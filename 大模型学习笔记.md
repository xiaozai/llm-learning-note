# 1. 数据流程
考察点：项目是否真实落地，数据来源、清洗、验证流程是否清晰，能否从0到1构建数据体系。

## 1. SFT冷启动时数据集构造需要注意哪些因素？为什么要做数据清洗与均衡采样？
Answer:
目标： SFT冷启动阶段的目标是激发模型已经具备的对话、理解和**遵循指令**的能力，并**规范其输出格式**。

1. 注意事项：
- 质量远胜于数量
- 指令的多样性与复杂性： 覆盖度和粒度
- 答案的格式规范性：
  - 结构化
  - 拒绝回答的边界：构造一批“拒答类”数据。当用户询问模型不知道的信息或涉及敏感话题时，模型应该学会如何礼貌地拒绝，而不是强行编造。
- 避免知识泄露与时效性错配
  - 知识截止日期： 如果没有在数据中引导模型承认不确定性，会导致模型产生幻觉。
  - 解决方案： 在数据中标注知识来源，或在指令中明确要求模型基于给定的上下文回答。
- 防止灾难性遗忘
  - 保留通用能力：冷启动数据如果全是“问答”形式，可能会让模型忘记原本的续写或自然语言理解能力
  - 解决方案：在数据集中混入一定比例的通用语料（如高质量的文章，书籍片段）的续写任务，保持模型的基座能力

2. 数据清洗与均衡采样
- 数据清洗的必要性
  - 去除有害/偏见内容
  - 去除格式噪声： HTML标签、乱码，错误的标点符号和拼写等，防止模型学会输出无意义的符号，避免降低输出质量
  - 去除逻辑冲突：对于同一个问题，如果数据集中存在多种互相矛盾的优质答案，可能会让模型产生困惑，导致输出结果不稳定
- 均衡采样的必要性：解决**数据分布偏差**问题，防止模型“偏科”
  - 防止长尾遗忘
    - 问题：如果数据集中90%是常识问题，只有10%是代码生成，微调后的模型在写代码时，会倾向于像回答问题一样简短，而不是生成完整的代码块
    - 均衡采样： 通过上采样或下采样，人为地均衡各类任务的占比，确保模型对每一种核心能力都掌握的足够好
  - 防止模式坍塌
    - 问题：如果数据全都是有益的、积极的回答，模型会变得过于保守，无法处理一些需要批判性思维或略带幽默的场景
    - 均衡采样：需要确保数据中涵盖不同语气，不同长度以及不同情感色彩的回答，让模型的表达风格更加多样
  - 提升鲁棒性：
    - 问题： 如果训练数据中的指令都非常清晰，模型可能无法理解带有噪声的指令
    - 均衡采样：可以在数据中呼入一定比例的“带噪声”或“表达不清”的指令，并配上清晰的回答，教会模型如何处理模糊的输入

**总结**
在SFT冷启动阶段：
1. **数据集构造**的核心在于**质量**和**多样性**。数据必须准确、安全，且覆盖广泛的任务类型，以引导模型展现出全面的能力。
2. **数据清洗**是为了**净化输入**，移除有害和错误信息，确保模型学到的是正确的语言模式和价值观
3. **均衡采样**是为了**优化学习重心**，防止模型因为某些任务的数据过多而产生偏向，确保模型在各项能力上都能达到及格线以上，为后续的对齐工作打下坚实的基础。

## 2. 怎么构建SFT数据集，数据量多少，微调方式是什么？
Answer:
两条路径：**利用现有的高质量数据集** 和 **自建合成数据集**。

一个标准化的SFT数据集格式通常是JSONL文件，每条数据包含一个“指令-输入-输出”对，例如
`{"instruction": "...", "input": "question....", "output": "answer..."}`

- 数据量要求：质量远胜于数据量，用更少、更难、更高质量的数据，激发模型更强的能力。

- 数据量级的“阈值”： 一般为上万条

微调方式：
1. 全量微调 Full Fine-tuning
- 原理： 对预训练模型的所有参数进行更新训练
- 特点：
    - 优点：理论上能达到最好的效果，模型能最充分地适应新任务
    - 缺点： 计算资源消耗巨大，需要多卡GPU，训练时间长，且每个任务都需要保存一份完整的模型副本，存储成本高
    - 适用：算力充足，追求极致性能的场景
2. 参数高效微调 PEFT， Parameter-Efficient Fine-Tuning
- 原理：冻结预训练模型的大部分参数，仅插入或更新极少量的额外参数。
- 主流方法：
    - LoRA (Low-Rank Adaption)：在模型层旁添加低秩矩阵，只训练这些矩阵，效果接近全量微调，但可训练参数极少
    - Q-LoRA: LoRA的进阶版，先将模型量化到4-bit或8-bit以大幅度降低显存占用，再应用LoRA进行微调。
3. 分阶段微调 Multi-stage Fine-Tuning
- 首先在更高精度如BF16下对模型进行标准的SFT，以学习特定任务的行为。
- 然后，对SFT后的模型应用量化感知训练（QAT），将其精度降低至FP4或INT4等格式，同时通过训练来恢复因量化造成的精度损失。


## 3. SFT数据问题不够多样化怎么办？
Answer:
挖掘 - 合成 - 增强
1. 冷启动基座：先人工撰写100~200条覆盖核心场景的黄金数据，确保格式和质量绝对标杆。
2. 爆炸式扩展： 利用强模型 （如GPT-4，Gemini-2），以这200条为种子，生成5000~20000条合成数据，在Prompt中明确要求多样性和难度
3. 清洗与回过滤： 使用奖励模型或人工对合成数据进行打分，剔除低质量、重复或有害的数据
4. 增强混入：将最终数据的5%~10%替换为带有噪声或口语化表达的变体


## 4. 微调时的训练数据是怎么构建的？如何保证样本多样性和质量？

假设要训练一个10K样本的通用助手冷启动数据集，配比方案如下：
| 数据类比 | 占比 | 来源 | 目的 |
| --- | --- | --- | --- |
| 高质量人工数据 | 10% | 根据业务痛点精标 1000 条 | 确保核心能力上限，提供标杆 |
| 合成复杂推理数据 | 30% | 使用 COT 方法生成数学/逻辑题 | 提升模型思考深度 |
| 对话/通用指令数据 | 40% | 从开源精选集 (如 UltraChat, OpenOrca) 清洗后采样 | 覆盖泛化能力，提升对话流畅度 |
| 格式化数据 | 10% | 代码、JSON 生成、表格处理 | 提升结构化输出能力 |
| 安全/拒答数据 | 10% | 针对敏感词和越狱尝试的对抗性样本 | 确保模型边界安全 |

训练数据构建：
1. 数据源组成：多源融合策略
- 主干数据 60%~70%： 核心能力数据，如多轮对话、指令遵循
- 专项数据 20%~30%： 特定场景数据，如推理链、代码、角色扮演、多语言
- 通用数据 10%~20%： 基座能力保留数据，主要是纯文本续写，防止模型忘记语言建模能力
2. 数据格式转换：模板化封装
- 角色标记、系统提示词、最终格式 ShareGPT
3. 数据长度与注意力掩码处理
- 拼接：为了提升训练效率，通常会将多个短对话拼接成一个长的训练序列（如4096tokens），中间用`<eos>` 或 `<sep>` 分隔
- 损失计算掩码： 训练时，必须屏蔽掉 `User` 和 `System` 部分的损失，只计算 `Assistant` 回复部分的损失。通过生成一个损失掩码矩阵 loss mask 来实现。

如何保证样本的多样性？ -> **数据工程指标**来量化
1. 指令语义多样性
- Embedding聚类：使用Sentence-BERT等模型将指令转为向量，进行聚类分析。如果数据集中有太多样本挤在同一个簇（如都是“是什么”类问题），就需要人工或通过模型补充其他簇（如“为什么”或“怎么办”类问题）的样本
- 指令长度分布：检查短指令（<10词），中指令（10-50词），长指令（>50词）的比例。真实场景中长指令往往较少，如果数据集中长指令缺失，模型在处理复杂需求时会表现不佳
2. 任务类型多样性
- 分类覆盖： 建立Taxonomy（分类体系），确保数据覆盖主流任务类型
    - 知识问答、创意写作、代码生成、逻辑推理、摘要总结、翻译、改写润色、多轮对话等
- **拒绝回答样本**： 必须有意识地加入安全拒答类和边界拒答类数据，让模型学会区分能回答和不能回答的问题
3. 领域多样性
- 对于垂直领域模型，需要检查数据在垂直领域内的细分类别是否平衡

如何保证样本质量？
1. 源头控制
- 人工标注规范
- 筛选教师模型输出
2. 自动化质量过滤（通过算法进行筛查）
- 低文本熵过滤：去除重复的、无意义的文本
- 困惑度过滤：用一个成熟的模型计算数据的困惑度，困惑度极高，说明语法混乱或逻辑不同，可以剔除；困惑度极低，说明过于简单，缺乏训练价值，也可以适当剔除；
- **N-gram**去重：在数据集层面，移除高度重复的n-gram片段，防止模型反复学习同样的句式，导致输出单一。
3. 质量评估与清洗
- 规则检查：检查答案是否包含不完整的句子，是否包含乱码，是否答非所问等等
- 多维度的Reward Model打分：训练一个专门的评估模型，从有用性、诚实性、无害性等多个维度给数据样本打分，过滤掉低分样本。

## 5. 如何从用户行为日志数据中抽取训练对话的？有没有做过归一化或事件抽象？
**数据清洗、会话切分、归一化与事件抽象**
1. 数据清洗与预处理
- 去噪，剔除异常值： 过滤掉包含错误格式、噪声或异常行为的记录，确保数据质量
- 去重： 移除重复的会话记录，避免模型学习到重复的模式
- 缺失值处理： 填充或删除缺失的用户ID、会话ID、时间戳等字段
- 异常值检测： 识别并处理异常的用户行为，如异常的时间间隔、异常的输入输出长度等
2. 会话切分
- 基于时间间隔： 假设用户在10分钟内没有任何交互，就将这10分钟内的所有记录作为一个会话。
- 基于用户切换： 当用户ID发生切换时，将之前的记录作为一个会话。
3. 归一化
- 统一时间格式： 将所有时间戳转换为统一的时间格式（如ISO 8601），方便后续处理。
- 标准化输入输出： 对用户输入和助手回复进行标准化，如去除多余空格、统一大小写等。
4. 事件抽象
- 提取关键事件： 从用户行为中提取出关键事件，如用户问题、助手回答、系统指令等。
- 构建事件序列： 将这些事件按时间顺序排列，形成一个事件序列。

进阶思考： 如何自动化挖掘训练样本？
1. 利用用户反馈（交互式学习），当用户对系统回答不满意并进行纠正时，将这轮对话作为训练样本。
2. 从用户行为中挖掘异常模式： 例如，用户频繁切换主题、重复问题等，这些都是模型需要学习到的异常行为。
3. 利用模型生成的样本： 模型在训练过程中，会生成一些样本。可以将这些样本作为训练数据，特别是那些模型认为是有价值的样本。
4. 多模型一致性检验，同时用多个模型对同一条用户日志进行解析，对比不同模型的输出，筛选出一致的样本作为训练数据。

## 6. 数据集构造的自动化流程是怎么实现的？
1. 数据采集
- 从用户行为日志中提取会话记录
- 从用户反馈中挖掘异常模式
2. 数据清洗与预处理
- 去噪，剔除异常值
- 去重
- 缺失值处理
- 异常值检测
3. 会话切分
- 基于时间间隔
- 基于用户切换
4. 归一化
- 统一时间格式
- 标准化输入输出
5. 事件抽象
- 提取关键事件
- 构建事件序列
6. 样本挖掘
- 利用用户反馈
- 从用户行为中挖掘异常模式
- 利用模型生成的样本
- 多模型一致性检验
7. 样本存储
- 存储为JSONL格式，每个样本占一行，包含用户ID、会话ID、时间戳、用户输入、助手回复等字段。

## 7. 为何选择vLLM部署服务？KV-cache如何帮助推理加速？你自己做过哪些优化？
1. 基于vLLM的推理服务： 选择vLLM作为推理服务，因为它在处理长序列时具有较高的效率和吞吐量。
2. PagedAttention: KV-cache优化： 利用vLLM的KV-cache机制，缓存已计算的键值对，避免重复计算，显著提高推理速度。
- 分页缓存： 将缓存分为多个页面，每个页面包含一定数量的键值对。当模型需要计算新的键值对时，先检查缓存中是否已存在，若存在则直接使用，避免重复计算。
- 动态更新： 模型在运行过程中，会动态更新缓存中的键值对。当缓存满时，会根据最近最少使用（LRU）策略替换旧的键值对。
3. 批量处理： 对输入进行批量处理，减少模型调用次数，提高效率。
4. 异步处理： 采用异步模型，充分利用服务器资源，处理多个请求。

**自己可以尝试的优化**
- 模型并行： 利用多GPU并行计算，提高模型推理速度。
- 缓存优化： 对常用的输入进行缓存，避免重复计算。
- 批量处理： 对输入进行批量处理，减少模型调用次数，提高效率。
- 显存碎片整理： 及时释放不再使用的显存，避免内存碎片问题。
- KV量化： 对模型的键值对进行量化，减少显存占用，提高推理速度。


## 8. RAG的chunk划分策略是什么？
1. 基于内容的划分： 按段落、句子或固定长度等方式划分文档内容，每个部分作为一个chunk。
2. 基于语义的划分： 利用语义分析技术，将文档内容根据语义关系进行划分，确保每个chunk的语义连贯。
3. 混合划分： 结合基于内容和基于语义的划分方法，根据具体场景选择合适的划分策略。


## 9. 在构造偏好数据时，用同一个prompt采样多个response，怎么保证多样性？
1. 采样参数： 调整温度（temperature）和Top-p（nucleus sampling）参数，控制模型生成的随机性。较高的温度会增加随机性，而较低的Top-p会限制生成的 tokens 范围，从而增加多样性。
2. 不同的种子（seed）： 为每个采样设置不同的随机种子，确保每次生成的结果不同。
3. 人工筛选： 对生成的 response 进行人工筛选，保留符合预期的样本，过滤掉不相关或重复的样本。


## 10. 在构建偏好数据时用了聚类方法筛选高质量样本，为什么没选KMeans?
1. KMeans不符合偏好样本空间
- 簇形状假设过强： KMeans假设样本分布在欧氏空间中，而偏好样本空间可能是一个高维空间，簇形状假设过强，导致聚类结果不符合预期。
- 对异常值敏感： KMeans对异常值敏感，可能会将异常值分配到错误的簇中。
- 需要预设K： K值很难确定
2. 聚类方法
- 基于距离的聚类： 如KMeans、DBSCAN等，根据样本之间的距离进行聚类。
- 基于密度的聚类： 如DBSCAN、OPTICS等，根据样本的密度分布进行聚类。
- 基于层次的聚类： 如Agglomerative Clustering、Divisive Clustering等，通过合并或分裂样本进行聚类。


## 11. 自主构建的评估体系里如何分离知识幻觉与推理幻觉？
1. 知识幻觉： 模型生成的回复中包含了与训练数据中不存在的知识或信息。源于模型内部存储的事实性知识错误或缺失。
2. 推理幻觉： 模型在生成回复时，基于训练数据中的模式进行推理，而不是基于真实的知识。源于模型在逻辑链条上的断裂或跳跃。

方法一：基于知识图谱的反向验证
1. 构建知识图谱： 利用已有的知识库，构建一个知识图谱，将知识实体和关系表示为图结构。
2. 验证回复： 对模型生成的回复，利用知识图谱进行反向验证。检查回复中是否包含了训练数据中不存在的知识或信息。
3. 筛选样本： 根据验证结果，筛选出符合预期的样本作为训练数据。

方法二：干扰项注入与上下文遮蔽
1. 干扰项注入： 为模型输入添加随机干扰项，例如添加噪声、替换某些 tokens 等，以破坏模型对真实知识的依赖。
2. 上下文遮蔽： 对模型输入中的关键信息进行遮蔽，例如替换为占位符或特殊 tokens，以防止模型基于上下文进行推理。
3. 评估模型输出： 对注入干扰项和遮蔽上下文后的模型输出进行评估，判断是否存在知识幻觉或推理幻觉。

方法三：构建专门的“反常识/伪知识”数据集
1. 收集数据： 从已有的知识库中收集包含常识性错误或伪知识的样本。
2. 标注数据： 对收集到的样本进行人工标注，标记出其中的知识幻觉和推理幻觉。
3. 训练模型： 利用标注好的数据集，训练一个分类模型，用于判断模型生成的回复是否存在知识幻觉或推理幻觉。

## 12. 做Prompt优化时，是如何判断优化后的Prompt在Agent推理链路中性能提升的？用什么指标来衡量？
**控制变量与A/B测试**
1. 控制变量： 确保其他因素（如模型、数据集等）保持不变，仅关注Prompt优化的效果。
2. A/B测试： 对优化前和优化后的Prompt分别进行A/B测试，对比模型生成的回复质量。常用的指标包括回复质量（如BLEU、ROUGE等）、用户满意度等。

**衡量指标**
1. 结果指标
- 用户满意度： 收集用户对模型回复的满意度调查，评估用户是否对回复满意。
- 任务完成率： 统计模型在完成特定任务（如信息检索、问题回答等）的样本比例。
- 回复质量（如BLEU、ROUGE等）： 评估模型生成的回复与人工标注的参考回复之间的相似度
- 答案准确率： 统计模型在回答问题时，正确回答的样本比例。
- 鲁棒性得分： 评估模型在处理不同输入时的稳定性和可靠性，常用的指标包括模型在不同领域、不同任务上的表现。
2. 过程指标
- 工具调用准确率： 统计模型在调用外部工具（如搜索引擎、数据库等）时，成功调用的比例。
- 无效轨迹率 / 死循环率： 统计模型在生成回复时，出现无效轨迹（如重复调用相同工具、陷入循环等）的样本比例。
- 推理效率： 评估模型在处理单个请求时的响应时间，包括模型推理时间和后处理时间，平均推理步数
- 轨迹连贯性得分： 评估模型生成的回复是否连贯，是否符合逻辑链条。常用的指标包括BLEU、ROUGE等。
3. 效率与经济指标
- 推理成本： 评估模型在处理单个请求时的成本，包括模型推理成本和后处理成本。
- 系统吞吐量： 评估模型在单位时间内处理的请求数量，通常用每秒请求数（Requests per Second, RPS）表示。

## 13. Agent的设计逻辑，问创新方法的实现
1. 认知架构创新： 从线性执行到元认知循环
- 线性执行： 传统的Agent执行流程是线性的，即按照固定的顺序执行任务。
- 元认知循环： 引入元认知循环，使Agent能够在执行任务时，根据任务需求和环境变化，动态调整执行策略。
- 实现快慢双系统
    - 快思考系统： 引入快思考系统，使Agent能够在短时间内对任务进行分析和决策，提高任务完成效率。
    - 慢思考系统： 引入慢思考系统，使Agent能够在处理复杂任务时，进行深度思考和推理，解决更复杂的问题。
2. 控制流创新：从顺序流到动态图
- 顺序流： 传统的控制流是顺序执行的，即按照固定的顺序执行任务。
- 动态图： 引入动态图，使Agent能够根据任务需求和环境变化，动态调整控制流。
- 实现方式：递归规划与自我修订
    - 递归规划： 利用递归函数，根据任务需求和环境变化，动态生成控制流。
    - 自我修订： 利用Self-Reflection机制，使Agent能够在执行过程中，根据反馈和错误，自我修正控制流。
3. 记忆机制创新： 从短期缓存到结构化记忆
- 短期缓存： 传统的记忆机制是短期缓存，即只保留最近的任务信息。
- 结构化记忆： 引入结构化记忆，使Agent能够根据任务需求和环境变化，动态调整记忆内容。
- 实现方式： 利用向量数据库，将任务信息和环境信息表示为向量，存储在数据库中。在执行任务时，根据任务需求和环境变化，从数据库中检索相关信息。
4. 协作模式创新：从单打独斗到多智能体辩论
- 单打独斗： 传统的协作模式是单打独斗，即每个智能体独立执行任务。
- 多智能体辩论： 引入多智能体辩论，使Agent能够在处理复杂任务时，与其他智能体合作，解决更复杂的问题。
- 实现方式： 利用分布式系统，将多个智能体部署在不同的节点上，通过通信协议（如RPC）实现智能体之间的协作。
5. **推理范式创新： 代码即思维**
- 传统逻辑： 自然语言思考
- 创新逻辑：编程语言思考
- 实现方法： 代码链

## 14. 如果召回的答案不是想要的，该怎么处理？
1. 检索阶段的阈值与重排： 调整检索阶段的阈值，筛选出与任务相关的知识。同时，利用重排模型，对检索到的知识进行排序，提高与任务相关的知识的排名。
2. Prompt内嵌入校验逻辑： 在Prompt中嵌入校验逻辑，判断模型回复是否符合任务要求。如果不符合，触发重新召回机制。
- 重新召回： 利用RAG技术，重新召回与任务相关的知识。
3. 基于识别结果的多路径决策
- 完全无关 -> 友好拒绝与澄清
- 部分相关 -> 回答已知部分 + 指出未知
- 检索失败 -> 意图改写与二次检索
4. 引入评估-行动循环
5. 主动追问与澄清
6. 反馈机制： 引入用户反馈机制，收集用户对模型回复的满意度调查，评估用户是否对回复满意。
- 模型调整： 根据用户反馈，调整模型参数，优化模型生成的回复质量。

# 2. 技术选型与深度对比
考察点：是否具备独立技术判断力，能清晰阐述不同方案优劣及特定场景下的选型依据。

## 1. 对比Qwen3与DeepSeek-R1的模型架构，他们在设计上有哪些关键的技术路径差异？
1. Qwen3 通过动态稀疏激活和混合专家架构在保持性能的同时大幅度降低推理成本
- **核心架构**： 动态稀疏混合专家架构 Dynamic Sparse MoE, 专家激活比例动态调整至50%~70%， 175B参数规模下推理FLOPS降低42%
- **注意力机制**： Transformer-XL 改进架构 + 动态位置编码 + 滑动窗口注意力，支持256K tokens超长上下文
- 推理优化：混合精度量化（FP16/INT8动态切换）+ 投机解码，推理吞吐量提升3倍，INT8模式下精度损失<1%
- 训练优化：三阶段预训练（通用基础→推理增强→长上下文）+ 四阶段后训练（Long-CoT冷启动→推理RL→思维模式融合→通用RL）
- 数据处理：36T tokens预训练语料，覆盖119种语言；通过Qwen2.5系列模型大规模合成数据，并采用双阶段过滤（查询+响应）确保质量
- 多模态能力： 原生支持文本、图像、语音、视频四模态，视觉编码器采用改进Swin Transformer V2，COCO数据集mAP达62.3
- 思维控制： “混合思维”模式：同一模型可切换即时回答与逐步推理，支持“思维预算”控制
2. DeepSeek-R1 聚焦于深度推理能力的突破，通过大规模强化学习和独特的注意力机制优化复杂任务的逻辑链条
- **核心架构**：大规模稀疏MoE架构，总参数671B，但每个Token仅激活37B参数， 256个路由专家中激活8个+共享专家
- **注意力机制**： 多头潜在注意力（MLA），显著减小KV缓存大小，但注意力算术成本较高（在4K上下文时注意力FLOP与参数FLOP相当）
- 推理优化：动态精度调整（简单任务INT8提速3倍，复杂任务FP16保精度）+ GRPO强化学习，推理速度较密集架构提升40%
- 训练优化：两阶段训练：DeepSeek-V3预训练（14.8T tokens，266万H800小时）+ R1强化学习（基于规则的GRPO算法）
- 数据处理： 预训练数据14.8T tokens，强化学习阶段基于可验证任务（代码单元测试、数学答案校验）构建奖励信号
- 多模态能力： 支持文本、图像、代码三模态联合训练，视觉问答VQA v2.0准确率92.3%，但需分模态调用API
- 思维控制： **专注深度推理**，通过RL训练出完整的思维链（CoT）能力，在数学、逻辑等任务中展现长链推理

## 2. GRPO和PPO在RLHF中的核心区别是什么？为什么选择GRPO而不是PPO？
- GRPO: Group Relative Policy Optimization 群体相对策略优化
- PPO: Proximal Policy Optimization 近端策略优化

**核心区别**： Critic （评价模型） vs. 无 Critic

**PPO** 是一种典型的Actor-Critic架构
- 工作方式：
  - Actor 策略模型： 负责生成文本
  - Critic 评价模型： 用于评估某个状态下未来的预期总收益，用于计算优势函数
- 缺点：
  - 显存开销大： 同时加载Actor和Critic两个大模型
  - 训练不稳定： Critic作为辅助模型，其本身的预测误差会引入偏差，导致训练过程不够稳定，且调参较为复杂

**GRPO** 无Critic，依靠群体采样
- 工作方式：
  - 针对当前状态（Prompt），让旧策略生成一组多个输出样本
  - 利用这些样本的得分计算出群体的均值和标准差
  - 用这个群体的统计量来对单个样本的奖励进行标准化，从而判断某个输出在群体中的相对优劣
- 优点：
  - 节省资源
  - 降低偏差：不再依赖一个可能不准确的Critic模型来计算优势，而是通过实际采样结果来进行比较，使优势估计更贴近真实情况

**选择GRPO的原因**
1. 显著降低显存占用与训练成本
2. 更稳定的训练信号：
- PPO的Critic模型使动态学习的，容易出现Reward Hacking；
- GRPO 不依赖于绝对数值，在奖励函数尺度不一的情况下更加稳健
3. 更符合语言生成的采样特性
4. 简化RLHF流程


## 3. LoRA和全参微调相比，在推荐场景下各自的优缺点是什么？
- LoRA （Low-Rank Adaption， 低秩适应）
  - 核心思想是冻结预训练的原有参数，然后在模型的某些层旁添加一个可训练的低秩矩阵，训练时只更新这个小矩阵
  - 优点：轻量级多场景部署（大模型统一底座+小插件灵活适配）， 缓解灾难性遗忘，训练更稳定，收敛更快，，推理无额外延迟
  - 缺点：低秩假设的限制，LoRA的表达能力可能不如全参微调，选择插入位置有门槛，难以改变底层特征
- Full Fine-tuning 全参微调
  - 优点： 理论上限高，无信息丢失，简单直接
  - 缺点： 灾难性遗忘，训练成本极高，存储成本巨大， 过拟合风险


## 4. DPO相比SFT，有哪些优劣？他在Agent任务上效果提升明显吗？

**DPO （Direct Preference Optimization，直接偏好优化）**

**核心逻辑的差异**
1. SFT： 模仿“正确答案”， 本质是最大似然估计，通过大量人工标注的“输入-输出”对，训练模型去模仿标准答案。
2. DPO： 区分“好与坏”， 基于偏好的优化算法，，直接利用偏好数据优化策略，给定输入，模型要更倾向与生成人类偏好的回答，同时疏远不被偏好的回答。

**DPO相比SFT的优势**
1. 更精准的对齐：
- SFT适合学习”格式“和”知识”，无法理解“为什么这个回答好”；
- DPO显示地告诉模型”好回答“和”坏回答”的区别，鼓励模型在概率空间中拉大两者的距离
2. 缓解曝光偏差 Exposure Bias
- SFT训练时总以“正确答案”作为上下文进行预测，但在推理时，模型用自己的预测作为上下文，一旦早期预测有偏差，误差会积累
- DPO的数据通常来自模型自身的采样（或旧策略的采样），模型在训练时就已经接触过自己可能犯的错误，并学习如何修正或避免，使得推理时更加鲁棒
3. 降低不良输出概率
- SFT 很难处理“拒答数据”
- DPO包含负样本，通过（Chosen，Rejected）的成对数据，模型能清晰地学会抑制不良输出的概率，实现“不做什么”的精确控制。


## 5. 为什么要在推荐系统引入RAG？知识库的数据来源和构建流程是怎样的？

## 6. 为什么选用这个优化器AdamW,它比SGD好在哪里？

## 7. 为什么选择这个特定Qwen2.5-VL-7B-Instruct的多模态大模型，而不是其他的？

## 8. 为什么选用DeepSeek，了解deepseek-R1吗？

## 9. 什么场景下用SFT，什么场景下用RL？

## 12. 项目中用了LangGraph实现多工具调用链路，相比纯Prompt有什么优势？

## 13. vLLM的核心优势是什么？它是如何通过PagedAttention提升显存利用率的？

## 14. 了解哪些agent开发框架，例如langchain和LlamaIndex,他们核心应用场景有何不同？

## 15. 如何做微调的？直接用PEFT库，还是用LlamaFactory做的？

## 16. 问Agent的工具tool的设计，是否是workflow形式？

## 17. 多工具调用中如何用DAG实现并行调度优化？

## 18. R1的MLA是如何节约KV cache的？

## 19. 多模态输入： 如果要做电商agent，你会选择哪些模态的信息作为输入？比如文本评论，图像，视频，购买记录？

## 21. 长上下文处理：超长上下文是怎么实现的？如KIMI的上下文窗口是128K，你是如何处理的？

## 22. 幻觉与幅度： 什么是大模型的幻觉？如何缓解？为什么会有幅度问题，业内有哪些解决办法？

## 23. 你在大模型训练中遇到的困难，如何解决？

## 24. 如果让你自己设计一个Agent，你会怎么做？为什么？

## 25. 你在项目中使用过DPO吗？和PPO相比，他有什么优缺点？请对比两者损失函数形式，并解释GRPO在训练稳定性上的优势

## 26. 微调项目是如何模型选型的？

## 27. vLLM中使用的技术是否熟悉？

## 28. 什么场景需要GraphRAG，他的好处是什么？

## 29. 项目中利用LangGraph来编排多工具调用链路，与纯Prompt工程方法相比，这种框架带来了哪些核心优势？


# 3. 工程化与系统设计
考察点： 解决实际工程难题的能力，关注性能，资源利用率，稳定性及长上下文处理等硬指标

1. 当输入超过模型上下文长度时，有哪些主流解决方案？

2. 在实际本地部署vLLM服务时，如何权衡上下文长度与显存占用的关系？是否应用过量化或动态批处理等技术？

3. KV Cache在长上下文推理中可能存在污染问题。你们是否设计了相应的缓存隔离或清理机制？

4. 本地部署时，如何平衡vLLM的上下文长度和显存占用？

5. 请手推一下MSE的梯度回传过程

6. Multi-Head Attention为什么要切分成多头？他能提高计算效率吗？请推导一下公式。

7. LoRA的原理是什么？ A、B矩阵是如何初始化的？

8. 项目里的Modular Agent，你能讲讲它是如何实现多步规划的吗？项目里提到了多个工具调用链路，调度策略是如何设计的？是否有异常fallback策略？

9. Agent评估体系包括哪些维度？如何衡量planning能力 vs hallucination rate？

10. 场景题：加入一个Agent推理链路包含3个工具+高频请求，系统整体延迟较高，你会如何优化？

11. GraphRAG的难点是什么？如何应对增量场景？

12. 客户输入一个软件或网页界面截图，如何通过RAG的方式帮助用户了解界面的每一个组件作用，相似的组件如图片框和视频框如何区分？

13. 如何搭建一个RAG链路来处理专业领域知识？ 如医疗、法律？

14. 大规模Agent系统在多线程、多进程场景下的资源调度策略如何设计？

15. 在高并发查询Agent系统中，你会如何优化召回和生成阶段的延迟？

16. 如果要在GPU资源有限的条件下同事提供推理和微调服务，如何做资源分配和任务调度以保证时延和吞吐？

17. LangChain的memory默认机制在多用户并发中怎么做隔离？你是如何保证线程安全的？

18. 训练LoRA模型时，你是如何选择冻结层的？依据是什么？

19. 微调Llama2你是怎么选择训练样本的？清洗的逻辑是什么？你有没有观察到哪些训练样本质量问题对模型行为有很大的影响？

20. kv cache是什么？为什么能极大地提升推理速度？

21. PPO的原理？从维护的四个model讲，再详细讲一下训练流程和损失函数各个参数含义

22. DPO和PPO的区别？

23. SFT冷启动时数据集构造需要注意哪些因素？为什么要做数据清洗与均衡采样？

24. 怎么构建SFT数据集，数据量是多少，微调方式是什么？

25. SFT数据问题不够多样化怎么办？

26. 微调时的训练数据是怎么构建的？如何保证样本多样性和质量？

# 4. 项目难点与故障排查
考察点：遇到困难时的分析和解决能力，是否独立找到根因并提出解决方案

1. 微调Qwen时发现验证集loss震荡的可能原因

2. 如果在训练DPO的过程中，正例和负例的loss都在下降，该如何解决？

3. 遇到过灾难性遗忘吗？怎么缓解的？

4. 模型量化时遇到激活值异常溢出如何调试？

5. 如何召回的答案不是想要的，该怎么处理？

6. 训练SFT模型时loss出现距离震荡，你是如何诊断并解决的？

7. LLM重复生成内容的问题如何缓解？

8. 如果子agent回复不对怎么办？反思？跳不出去怎么办？限制次数

9. Agent怎么评估效果

10. 你在大模型训练中遇到过的困难，如何解决？

11. 如果让你自己设计一个Agent， 你会怎么做？为什么？

# 5. 项目迭代与业务思考
考察点：是否具备宏观思考能力，能否将技术方案与业务价值结合，关注项目后续迭代方向

1. LLM4Rec对比传统搜推带来哪些收益？

2. 对llm4rec的后续迭代方向有什么见解？

3. 如果资源足够，有没有考虑过直接端到端训练

4. 你觉得目前大模型的上限在哪里？

5. 请介绍一下你的项目：目标是什么？用了什么基座模型？数据从哪来的？

6. RAG如果有噪声怎么办？

7. 如果Agent推理API需要低延迟响应，你会从哪些方面做系统级优化？

8. 后续迭代：对llm4rec的后续迭代方向有什么见解？

9. 业务结合： 你觉得在广告领域用大模型和传统的广告算法相比优缺点是什么？

10. 场景应用：介绍检索做的优化，具体追问子问题分解怎么做，有没有做意图识别？


# 6. 具体技术细节与参数调优
考察点：是否亲手跑过项目，具备扎实的调参经验和工程实现能力，理解每个参数对模型效果的具体影响

1. 请说明你设置的rank、alpha值，并分析他们对性能和手链速度的影响

2. LoRA微调秩设置的是多少

3. SFT的调参经验？说说你的经验

4. SFT使用的数据集，使用了多少张卡？SFT训练了多久？

5. SFT阶段如何避免对padding token计算loss？ 具体在代码中如何实现mask？

6. 你在大模型训练中遇到过的困难，如何解决？
