# 1. 大模型学习笔记
考察点：项目是否真实落地，数据来源、清洗、验证流程是否清晰，能否从0到1构建数据体系。

## 1. SFT冷启动时数据集构造需要注意哪些因素？为什么要做数据清洗与均衡采样？
Answer:
目标： SFT冷启动阶段的目标是激发模型已经具备的对话、理解和**遵循指令**的能力，并**规范其输出格式**。

1.1 注意事项：
    a. 质量远胜于数量
    b. 指令的多样性与复杂性： 覆盖度和粒度
    c. 答案的格式规范性：
        - 结构化
        - 拒绝回答的边界：构造一批“拒答类”数据。当用户询问模型不知道的信息或涉及敏感话题时，模型应该学会如何礼貌地拒绝，而不是强行编造。
    d. 避免知识泄露与时效性错配
        - 知识截止日期： 如果没有在数据中引导模型承认不确定性，会导致模型产生幻觉。
        - 解决方案： 在数据中标注知识来源，或在指令中明确要求模型基于给定的上下文回答。
    e. 防止灾难性遗忘
        - 保留通用能力：冷启动数据如果全是“问答”形式，可能会让模型忘记原本的续写或自然语言理解能力
        - 解决方案：在数据集中混入一定比例的通用语料（如高质量的文章，书籍片段）的续写任务，保持模型的基座能力

1.2 数据清洗与均衡采样
    a. 数据清洗的必要性
        - 去除有害/偏见内容
        - 去除格式噪声： HTML标签、乱码，错误的标点符号和拼写等，防止模型学会输出无意义的符号，避免降低输出质量
        - 去除逻辑冲突：对于同一个问题，如果数据集中存在多种互相矛盾的优质答案，可能会让模型产生困惑，导致输出结果不稳定
    b. 均衡采样的必要性：解决**数据分布偏差**问题，防止模型“偏科”
        - 防止长尾遗忘
            - 问题：如果数据集中90%是常识问题，只有10%是代码生成，微调后的模型在写代码时，会倾向于像回答问题一样简短，而不是生成完整的代码块
            - 均衡采样： 通过上采样或下采样，人为地均衡各类任务的占比，确保模型对每一种核心能力都掌握的足够好
        - 防止模式坍塌
            - 问题：如果数据全都是有益的、积极的回答，模型会变得过于保守，无法处理一些需要批判性思维或略带幽默的场景
            - 均衡采样：需要确保数据中涵盖不同语气，不同长度以及不同情感色彩的回答，让模型的表达风格更加多样
        - 提升鲁棒性：
            - 问题： 如果训练数据中的指令都非常清晰，模型可能无法理解带有噪声的指令
            - 均衡采样：可以在数据中呼入一定比例的“带噪声”或“表达不清”的指令，并配上清晰的回答，教会模型如何处理模糊的输入
**总结**
在SFT冷启动阶段：
1. **数据集构造**的核心在于**质量**和**多样性**。数据必须准确、安全，且覆盖广泛的任务类型，以引导模型展现出全面的能力。
2. **数据清洗**是为了**净化输入**，移除有害和错误信息，确保模型学到的是正确的语言模式和价值观
3. **均衡采样**是为了**优化学习重心**，防止模型因为某些任务的数据过多而产生偏向，确保模型在各项能力上都能达到及格线以上，为后续的对齐工作打下坚实的基础。



## 2. 怎么构建SFT数据集，数据量多少，微调方式是什么？
Answer:
两条路径：**利用现有的高质量数据集** 和 **自建合成数据集**。
一个标准化的SFT数据集格式通常是JSONL文件，每条数据包含一个“指令-输入-输出”对，例如
`{"instruction": "...", "input": "question....", "output": "answer..."}`

数据量要求：质量远胜于数据量，用更少、更难、更高质量的数据，激发模型更强的能力。
数据量级的“阈值”： 一般为上万条

微调方式：
1. 全量微调 Full Fine-tuning
- 原理： 对预训练模型的所有参数进行更新训练
- 特点：
    - 优点：理论上能达到最好的效果，模型能最充分地适应新任务
    - 缺点： 计算资源消耗巨大，需要多卡GPU，训练时间长，且每个任务都需要保存一份完整的模型副本，存储成本高
    - 适用：算力充足，追求极致性能的场景
2. 参数搞笑微调 PEFT， Parameter-Efficient Fine-Tuning 
- 原理：冻结预训练模型的大部分参数，仅插入或更新极少量的额外参数。
- 主流方法：
    - LoRA (Low-Rank Adaption)：在模型层旁添加低秩矩阵，只训练这些矩阵，效果接近全量微调，但可训练参数极少
    - Q-LoRA: LoRA的进阶版，先将模型量化到4-bit或8-bit以大幅度降低显存占用，再应用LoRA进行微调。
3. 分阶段微调 Multi-stage Fine-Tuning
- 首先在更高精度如BF16下对模型进行标准的SFT，以学习特定任务的行为。
- 然后，对SFT后的模型应用量化感知训练（QAT），将其精度降低至FP4或INT4等格式，同时通过训练来恢复因量化造成的精度损失。

## 3. SFT数据问题不够多样化怎么办？
Answer:
挖掘 - 合成 - 增强
1. 冷启动基座：先人工撰写100~200条覆盖核心场景的黄金数据，确保格式和质量绝对标杆。
2. 爆炸式扩展： 利用强模型 （如GPT-4），以这200条为种子，生成5000~20000条合成数据，在Prompt中明确要求多样性和难度
3. 清洗与回过滤： 使用奖励模型或人工对合成数据进行打分，剔除低质量、重复或有害的数据
4. 增强混入：将最终数据的5%~10%替换为带有噪声或口语化表达的变体


## 4. 微调时的训练数据是怎么构建的？如何保证样本多样性和质量？

假设要训练一个10K样本的通用助手冷启动数据集，配比方案如下：
| 数据类比 | 占比 | 来源 | 目的 |
| --- | --- | --- | --- |
| 高质量人工数据 | 10% | 根据业务痛点精标 1000 条 | 确保核心能力上限，提供标杆 |
| 合成复杂推理数据 | 30% | 使用 COT 方法生成数学/逻辑题 | 提升模型思考深度 |
| 对话/通用指令数据 | 40% | 从开源精选集 (如 UltraChat, OpenOrca) 清洗后采样 | 覆盖泛化能力，提升对话流畅度 |
| 格式化数据 | 10% | 代码、JSON 生成、表格处理 | 提升结构化输出能力 |
| 安全/拒答数据 | 10% | 针对敏感词和越狱尝试的对抗性样本 | 确保模型边界安全 |

训练数据构建：
1. 数据源组成：多源融合策略
- 主干数据 60%~70%： 核心能力数据，如多轮对话、指令遵循
- 专项数据 20%~30%： 特定场景数据，如推理链、代码、角色扮演、多语言
- 通用数据 10%~20%： 基座能力保留数据，主要是纯文本续写，防止模型忘记语言建模能力
2. 数据格式转换：模板化封装
- 角色标记、系统提示词、最终格式 ShareGPT
3. 数据长度与注意力掩码处理
- 拼接：为了提升训练效率，通常会将多个短对话拼接成一个长的训练序列（如4096tokens），中间用`<eos>` 或 `<sep>` 分隔
- 损失计算掩码： 训练时，必须屏蔽掉 `User` 和 `System` 部分的损失，只计算 `Assistant` 回复部分的损失。通过生成一个损失掩码矩阵 loss mask 来实现。

如何保证样本的多样性？ -> **数据工程指标**来量化
1. 指令语义多样性
- Embedding聚类：使用Sentence-BERT等模型将指令转为向量，进行聚类分析。如果数据集中有太多样本挤在同一个簇（如都是“是什么”类问题），就需要人工或通过模型补充其他簇（如“为什么”或“怎么办”类问题）的样本
- 指令长度分布：检查短指令（<10词），中指令（10-50词），长指令（>50词）的比例。真实场景中长指令往往较少，如果数据集中长指令缺失，模型在处理复杂需求时会表现不佳
2. 任务类型多样性
- 分类覆盖： 建立Taxonomy（分类体系），确保数据覆盖主流任务类型
    - 知识问答、创意写作、代码生成、逻辑推理、摘要总结、翻译、改写润色、多轮对话等
- 拒绝回答样本： 必须有意识地加入安全拒答类和边界拒答类数据，让模型学会区分能回答和不能回答的问题
3. 领域多样性
- 对于垂直领域模型，需要检查数据在垂直领域内的细分类别是否平衡

如何保证样本质量？
1. 源头控制
- 人工标注规范
- 筛选教师模型输出
2. 自动化质量过滤（通过算法进行筛查）
- 低文本熵过滤：去除重复的、无意义的文本
- 困惑度过滤：用一个成熟的模型计算数据的困惑度，困惑度极高，说明语法混乱或逻辑不同，可以剔除；困惑度极低，说明过于简单，缺乏训练价值，也可以适当剔除；
- **N-gram**去重：在数据集层面，移除高度重复的n-gram片段，防止模型反复学习同样的句式，导致输出单一。
3. 质量评估与清洗
- 规则检查：检查答案是否包含不完整的句子，是否包含乱码，是否答非所问等等
- 多维度的Reward Model打分：训练一个专门的评估模型，从有用性、诚实性、无害性等多个维度给数据样本打分，过滤掉低分样本。

## 5. 训练过程中数据来自用户行为日志，你是如何从这些数据中抽取训练对话的？有没有做过归一化或事件抽象？

## 6. 在你的问答Agent项目中，数据集构造的自动化流程是怎么实现的？

## 7. 你说你服务部署在vLLM上，为何选择它？KV-cache如何帮助推理加速？你自己做过哪些优化？

## 8. 微调数据集的结构是怎样的？有多少条？数据清洗和有效性验证时怎么做的？

## 9. RAG的chunk划分策略是什么？

## 10. 项目中的数据规模多大？SFT数据是如何清洗和构建的？

## 11. 在构造偏好数据时，你是用同一个propt采样多个response吗？怎么保证多样性？

## 12. 在构建偏好数据是，你才用了聚类方法筛选高质量样本，为什么没选KMeans?

## 13. 自主构建的评估体系里如何分离知识幻觉与推理幻觉？

## 14. 你做Prompt优化时，是如何判断优化后的Prompt在Agent推理链路中性能提升的？用什么指标来衡量？

## 15. 讲一下RAG项目的亮点

## 16. 问之前实习的Agent的设计逻辑，问创新方法的实现

## 17. 如果召回的答案不是想要的，该怎么处理？

# 2. 技术选型与深度对比
考察点：是否具备独立技术判断力，能清晰阐述不同方案优劣及特定场景下的选型依据。

1. 对比Qwen3与DeepSeek-R1的模型架构，他们在设计上有哪些关键的技术路径差异？

2. GRPO和PPO在RLHF中的核心区别是什么？为什么选择GRPO而不是PPO？

3. LoRA和全参微调相比，在推荐场景下各自的优缺点是什么？

4. RQ-VAE和VQ-VQE有什么不同？

5. DPO相比SFT，有哪些优劣？他在Agent任务上效果提升明显吗？

6. 为什么要在推荐系统引入RQ-VAE？RQ-VAE怎么解决坍缩问题？

7. 为什么要在推荐系统引入RAG？

8. 为什么在推荐系统中引入RAG？知识库的数据来源和构建流程是怎样的？

9. 为什么选用这个优化器AdamW,它比SGD好在哪里？

10. 你们为什么选择这个特定的多模态大模型，而不是其他的？

11. 为什么选用DeepSeek，了解deepseek-R1吗？

12. 什么场景下用SFT，什么场景下用RL？

13. 项目中用了LangGraph实现多工具调用链路，相比纯Prompt有什么优势？

14. vLLM的核心优势是什么？它是如何通过PagedAttention提升显存利用率的？

15. 了解哪些agent开发框架，例如langchain和LlamaIndex,他们核心应用场景有何不同？ 

16. 如何做微调的？直接用PEFT库，还是用LlamaFactory做的？

17. 问Agent的工具tool的设计，是否是workflow形式

18. 多工具调用中如何用DAG实现并行调度优化？

19. R1的MLA是如何节约KV cache的？

20. 多模态输入： 如果要做电商agent，你会选择哪些模态的信息作为输入？比如文本评论，图像，视频，购买记录？

21. 长上下文处理：超长上下文是怎么实现的？如KIMI

22. 幻觉与幅度： 什么是大模型的幻觉？如何缓解？为什么会有幅度问题，业内有哪些解决办法？

23. 数据构建：微调时的训练数据是怎么构建的？如何保证样本多样性和质量？

24. 你在大模型训练中遇到的困难，如何解决？

25. 如果让你自己设计一个Agent，你会怎么做？为什么？

26. 项目深挖： 尤其是Agent项目，从背景，动机，做法和结果等方面都问的非常仔细，聊了半小时（百度）

27. 你在项目中使用过DPO吗？和PPO相比，他有什么优缺点？请对比两者损失函数形式，并解释GRPO在训练稳定性上的优势

28. 微调项目是如何模型选型的？

29. vLLM中使用的技术是否熟悉

30. 为什么要在推荐系统中引入RAG？知识库的数据来源和构建流程是怎么样的？

31. RAG的chunk划分策略是什么？

32. 什么场景需要GraphRAG，他的好处是什么？

33. 项目中利用LangGraph来编排多工具调用链路，与纯Prompt工程方法相比，这种框架带来了哪些核心优势？


# 3. 工程化与系统设计
考察点： 解决实际工程难题的能力，关注性能，资源利用率，稳定性及长上下文处理等硬指标

1. 当输入超过模型上下文长度时，有哪些主流解决方案？

2. 在实际本地部署vLLM服务时，如何权衡上下文长度与显存占用的关系？是否应用过量化或动态批处理等技术？

3. KV Cache在长上下文推理中可能存在污染问题。你们是否设计了相应的缓存隔离或清理机制？

4. 本地部署时，如何平衡vLLM的上下文长度和显存占用？

5. 请手推一下MSE的梯度回传过程

6. Multi-Head Attention为什么要切分成多头？他能提高计算效率吗？请推导一下公式。

7. LoRA的原理是什么？ A、B矩阵是如何初始化的？

8. 项目里的Modular Agent，你能讲讲它是如何实现多步规划的吗？项目里提到了多个工具调用链路，调度策略是如何设计的？是否有异常fallback策略？

9. Agent评估体系包括哪些维度？如何衡量planning能力 vs hallucination rate？

10. 场景题：加入一个Agent推理链路包含3个工具+高频请求，系统整体延迟较高，你会如何优化？

11. GraphRAG的难点是什么？如何应对增量场景？

12. 客户输入一个软件或网页界面截图，如何通过RAG的方式帮助用户了解界面的每一个组件作用，相似的组件如图片框和视频框如何区分？

13. 如何搭建一个RAG链路来处理专业领域知识？ 如医疗、法律？

14. 大规模Agent系统在多线程、多进程场景下的资源调度策略如何设计？

15. 在高并发查询Agent系统中，你会如何优化召回和生成阶段的延迟？

16. 如果要在GPU资源有限的条件下同事提供推理和微调服务，如何做资源分配和任务调度以保证时延和吞吐？

17. LangChain的memory默认机制在多用户并发中怎么做隔离？你是如何保证线程安全的？

18. 训练LoRA模型时，你是如何选择冻结层的？依据是什么？

19. 微调Llama2你是怎么选择训练样本的？清洗的逻辑是什么？你有没有观察到哪些训练样本质量问题对模型行为有很大的影响？

20. kv cache是什么？为什么能极大地提升推理速度？

21. PPO的原理？从维护的四个model讲，再详细讲一下训练流程和损失函数各个参数含义

22. DPO和PPO的区别？

23. SFT冷启动时数据集构造需要注意哪些因素？为什么要做数据清洗与均衡采样？

24. 怎么构建SFT数据集，数据量是多少，微调方式是什么？

25. SFT数据问题不够多样化怎么办？

26. 微调时的训练数据是怎么构建的？如何保证样本多样性和质量？

# 4. 项目难点与故障排查
考察点：遇到困难时的分析和解决能力，是否独立找到根因并提出解决方案

1. 微调Qwen时发现验证集loss震荡的可能原因

2. 如果在训练DPO的过程中，正例和负例的loss都在下降，该如何解决？

3. 遇到过灾难性遗忘吗？怎么缓解的？

4. 模型量化时遇到激活值异常溢出如何调试？

5. 如何召回的答案不是想要的，该怎么处理？

6. 训练SFT模型时loss出现距离震荡，你是如何诊断并解决的？

7. LLM重复生成内容的问题如何缓解？

8. 如果子agent回复不对怎么办？反思？跳不出去怎么办？限制次数

9. Agent怎么评估效果

10. 你在大模型训练中遇到过的困难，如何解决？

11. 如果让你自己设计一个Agent， 你会怎么做？为什么？

# 5. 项目迭代与业务思考
考察点：是否具备宏观思考能力，能否将技术方案与业务价值结合，关注项目后续迭代方向

1. LLM4Rec对比传统搜推带来哪些收益？

2. 对llm4rec的后续迭代方向有什么见解？

3. 如果资源足够，有没有考虑过直接端到端训练

4. 你觉得目前大模型的上限在哪里？

5. 请介绍一下你的项目：目标是什么？用了什么基座模型？数据从哪来的？

6. RAG如果有噪声怎么办？

7. 如果Agent推理API需要低延迟响应，你会从哪些方面做系统级优化？

8. 后续迭代：对llm4rec的后续迭代方向有什么见解？

9. 业务结合： 你觉得在广告领域用大模型和传统的广告算法相比优缺点是什么？

10. 场景应用：介绍检索做的优化，具体追问子问题分解怎么做，有没有做意图识别？


# 6. 具体技术细节与参数调优
考察点：是否亲手跑过项目，具备扎实的调参经验和工程实现能力，理解每个参数对模型效果的具体影响

1. 请说明你设置的rank、alpha值，并分析他们对性能和手链速度的影响

2. LoRA微调秩设置的是多少

3. SFT的调参经验？说说你的经验

4. SFT使用的数据集，使用了多少张卡？SFT训练了多久？

5. SFT阶段如何避免对padding token计算loss？ 具体在代码中如何实现mask？

6. 你在大模型训练中遇到过的困难，如何解决？


