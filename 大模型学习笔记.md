# 1. 数据流程
考察点：项目是否真实落地，数据来源、清洗、验证流程是否清晰，能否从0到1构建数据体系。

## 1. SFT冷启动时数据集构造需要注意哪些因素？为什么要做数据清洗与均衡采样？
Answer:
目标： SFT冷启动阶段的目标是激发模型已经具备的对话、理解和**遵循指令**的能力，并**规范其输出格式**。

1. 注意事项：
- 质量远胜于数量
- 指令的多样性与复杂性： 覆盖度和粒度
- 答案的格式规范性：
  - 结构化
  - 拒绝回答的边界：构造一批“拒答类”数据。当用户询问模型不知道的信息或涉及敏感话题时，模型应该学会如何礼貌地拒绝，而不是强行编造。
- 避免知识泄露与时效性错配
  - 知识截止日期： 如果没有在数据中引导模型承认不确定性，会导致模型产生幻觉。
  - 解决方案： 在数据中标注知识来源，或在指令中明确要求模型基于给定的上下文回答。
- 防止灾难性遗忘
  - 保留通用能力：冷启动数据如果全是“问答”形式，可能会让模型忘记原本的续写或自然语言理解能力
  - 解决方案：在数据集中混入一定比例的通用语料（如高质量的文章，书籍片段）的续写任务，保持模型的基座能力

2. 数据清洗与均衡采样
- 数据清洗的必要性
  - 去除有害/偏见内容
  - 去除格式噪声： HTML标签、乱码，错误的标点符号和拼写等，防止模型学会输出无意义的符号，避免降低输出质量
  - 去除逻辑冲突：对于同一个问题，如果数据集中存在多种互相矛盾的优质答案，可能会让模型产生困惑，导致输出结果不稳定
- 均衡采样的必要性：解决**数据分布偏差**问题，防止模型“偏科”
  - 防止长尾遗忘
    - 问题：如果数据集中90%是常识问题，只有10%是代码生成，微调后的模型在写代码时，会倾向于像回答问题一样简短，而不是生成完整的代码块
    - 均衡采样： 通过上采样或下采样，人为地均衡各类任务的占比，确保模型对每一种核心能力都掌握的足够好
  - 防止模式坍塌
    - 问题：如果数据全都是有益的、积极的回答，模型会变得过于保守，无法处理一些需要批判性思维或略带幽默的场景
    - 均衡采样：需要确保数据中涵盖不同语气，不同长度以及不同情感色彩的回答，让模型的表达风格更加多样
  - 提升鲁棒性：
    - 问题： 如果训练数据中的指令都非常清晰，模型可能无法理解带有噪声的指令
    - 均衡采样：可以在数据中呼入一定比例的“带噪声”或“表达不清”的指令，并配上清晰的回答，教会模型如何处理模糊的输入

**总结**
在SFT冷启动阶段：
1. **数据集构造**的核心在于**质量**和**多样性**。数据必须准确、安全，且覆盖广泛的任务类型，以引导模型展现出全面的能力。
2. **数据清洗**是为了**净化输入**，移除有害和错误信息，确保模型学到的是正确的语言模式和价值观
3. **均衡采样**是为了**优化学习重心**，防止模型因为某些任务的数据过多而产生偏向，确保模型在各项能力上都能达到及格线以上，为后续的对齐工作打下坚实的基础。

## 2. 怎么构建SFT数据集，数据量多少，微调方式是什么？
Answer:
两条路径：**利用现有的高质量数据集** 和 **自建合成数据集**。

一个标准化的SFT数据集格式通常是JSONL文件，每条数据包含一个“指令-输入-输出”对，例如
`{"instruction": "...", "input": "question....", "output": "answer..."}`

- 数据量要求：质量远胜于数据量，用更少、更难、更高质量的数据，激发模型更强的能力。

- 数据量级的“阈值”： 一般为上万条

微调方式：
1. 全量微调 Full Fine-tuning
- 原理： 对预训练模型的所有参数进行更新训练
- 特点：
    - 优点：理论上能达到最好的效果，模型能最充分地适应新任务
    - 缺点： 计算资源消耗巨大，需要多卡GPU，训练时间长，且每个任务都需要保存一份完整的模型副本，存储成本高
    - 适用：算力充足，追求极致性能的场景
2. 参数高效微调 PEFT， Parameter-Efficient Fine-Tuning
- 原理：冻结预训练模型的大部分参数，仅插入或更新极少量的额外参数。
- 主流方法：
    - LoRA (Low-Rank Adaption)：在模型层旁添加低秩矩阵，只训练这些矩阵，效果接近全量微调，但可训练参数极少
    - Q-LoRA: LoRA的进阶版，先将模型量化到4-bit或8-bit以大幅度降低显存占用，再应用LoRA进行微调。
3. 分阶段微调 Multi-stage Fine-Tuning
- 首先在更高精度如BF16下对模型进行标准的SFT，以学习特定任务的行为。
- 然后，对SFT后的模型应用量化感知训练（QAT），将其精度降低至FP4或INT4等格式，同时通过训练来恢复因量化造成的精度损失。


## 3. SFT数据问题不够多样化怎么办？
Answer:
挖掘 - 合成 - 增强
1. 冷启动基座：先人工撰写100~200条覆盖核心场景的黄金数据，确保格式和质量绝对标杆。
2. 爆炸式扩展： 利用强模型 （如GPT-4，Gemini-2），以这200条为种子，生成5000~20000条合成数据，在Prompt中明确要求多样性和难度
3. 清洗与回过滤： 使用奖励模型或人工对合成数据进行打分，剔除低质量、重复或有害的数据
4. 增强混入：将最终数据的5%~10%替换为带有噪声或口语化表达的变体


## 4. 微调时的训练数据是怎么构建的？如何保证样本多样性和质量？

假设要训练一个10K样本的通用助手冷启动数据集，配比方案如下：
| 数据类比 | 占比 | 来源 | 目的 |
| --- | --- | --- | --- |
| 高质量人工数据 | 10% | 根据业务痛点精标 1000 条 | 确保核心能力上限，提供标杆 |
| 合成复杂推理数据 | 30% | 使用 COT 方法生成数学/逻辑题 | 提升模型思考深度 |
| 对话/通用指令数据 | 40% | 从开源精选集 (如 UltraChat, OpenOrca) 清洗后采样 | 覆盖泛化能力，提升对话流畅度 |
| 格式化数据 | 10% | 代码、JSON 生成、表格处理 | 提升结构化输出能力 |
| 安全/拒答数据 | 10% | 针对敏感词和越狱尝试的对抗性样本 | 确保模型边界安全 |

训练数据构建：
1. 数据源组成：多源融合策略
- 主干数据 60%~70%： 核心能力数据，如多轮对话、指令遵循
- 专项数据 20%~30%： 特定场景数据，如推理链、代码、角色扮演、多语言
- 通用数据 10%~20%： 基座能力保留数据，主要是纯文本续写，防止模型忘记语言建模能力
2. 数据格式转换：模板化封装
- 角色标记、系统提示词、最终格式 ShareGPT
3. 数据长度与注意力掩码处理
- 拼接：为了提升训练效率，通常会将多个短对话拼接成一个长的训练序列（如4096tokens），中间用`<eos>` 或 `<sep>` 分隔
- 损失计算掩码： 训练时，必须屏蔽掉 `User` 和 `System` 部分的损失，只计算 `Assistant` 回复部分的损失。通过生成一个损失掩码矩阵 loss mask 来实现。

如何保证样本的多样性？ -> **数据工程指标**来量化
1. 指令语义多样性
- Embedding聚类：使用Sentence-BERT等模型将指令转为向量，进行聚类分析。如果数据集中有太多样本挤在同一个簇（如都是“是什么”类问题），就需要人工或通过模型补充其他簇（如“为什么”或“怎么办”类问题）的样本
- 指令长度分布：检查短指令（<10词），中指令（10-50词），长指令（>50词）的比例。真实场景中长指令往往较少，如果数据集中长指令缺失，模型在处理复杂需求时会表现不佳
2. 任务类型多样性
- 分类覆盖： 建立Taxonomy（分类体系），确保数据覆盖主流任务类型
    - 知识问答、创意写作、代码生成、逻辑推理、摘要总结、翻译、改写润色、多轮对话等
- **拒绝回答样本**： 必须有意识地加入安全拒答类和边界拒答类数据，让模型学会区分能回答和不能回答的问题
3. 领域多样性
- 对于垂直领域模型，需要检查数据在垂直领域内的细分类别是否平衡

如何保证样本质量？
1. 源头控制
- 人工标注规范
- 筛选教师模型输出
2. 自动化质量过滤（通过算法进行筛查）
- 低文本熵过滤：去除重复的、无意义的文本
- 困惑度过滤：用一个成熟的模型计算数据的困惑度，困惑度极高，说明语法混乱或逻辑不同，可以剔除；困惑度极低，说明过于简单，缺乏训练价值，也可以适当剔除；
- **N-gram**去重：在数据集层面，移除高度重复的n-gram片段，防止模型反复学习同样的句式，导致输出单一。
3. 质量评估与清洗
- 规则检查：检查答案是否包含不完整的句子，是否包含乱码，是否答非所问等等
- 多维度的Reward Model打分：训练一个专门的评估模型，从有用性、诚实性、无害性等多个维度给数据样本打分，过滤掉低分样本。

## 5. 如何从用户行为日志数据中抽取训练对话的？有没有做过归一化或事件抽象？
**数据清洗、会话切分、归一化与事件抽象**
1. 数据清洗与预处理
- 去噪，剔除异常值： 过滤掉包含错误格式、噪声或异常行为的记录，确保数据质量
- 去重： 移除重复的会话记录，避免模型学习到重复的模式
- 缺失值处理： 填充或删除缺失的用户ID、会话ID、时间戳等字段
- 异常值检测： 识别并处理异常的用户行为，如异常的时间间隔、异常的输入输出长度等
2. 会话切分
- 基于时间间隔： 假设用户在10分钟内没有任何交互，就将这10分钟内的所有记录作为一个会话。
- 基于用户切换： 当用户ID发生切换时，将之前的记录作为一个会话。
3. 归一化
- 统一时间格式： 将所有时间戳转换为统一的时间格式（如ISO 8601），方便后续处理。
- 标准化输入输出： 对用户输入和助手回复进行标准化，如去除多余空格、统一大小写等。
4. 事件抽象
- 提取关键事件： 从用户行为中提取出关键事件，如用户问题、助手回答、系统指令等。
- 构建事件序列： 将这些事件按时间顺序排列，形成一个事件序列。

进阶思考： 如何自动化挖掘训练样本？
1. 利用用户反馈（交互式学习），当用户对系统回答不满意并进行纠正时，将这轮对话作为训练样本。
2. 从用户行为中挖掘异常模式： 例如，用户频繁切换主题、重复问题等，这些都是模型需要学习到的异常行为。
3. 利用模型生成的样本： 模型在训练过程中，会生成一些样本。可以将这些样本作为训练数据，特别是那些模型认为是有价值的样本。
4. 多模型一致性检验，同时用多个模型对同一条用户日志进行解析，对比不同模型的输出，筛选出一致的样本作为训练数据。

## 6. 数据集构造的自动化流程是怎么实现的？
1. 数据采集
- 从用户行为日志中提取会话记录
- 从用户反馈中挖掘异常模式
2. 数据清洗与预处理
- 去噪，剔除异常值
- 去重
- 缺失值处理
- 异常值检测
3. 会话切分
- 基于时间间隔
- 基于用户切换
4. 归一化
- 统一时间格式
- 标准化输入输出
5. 事件抽象
- 提取关键事件
- 构建事件序列
6. 样本挖掘
- 利用用户反馈
- 从用户行为中挖掘异常模式
- 利用模型生成的样本
- 多模型一致性检验
7. 样本存储
- 存储为JSONL格式，每个样本占一行，包含用户ID、会话ID、时间戳、用户输入、助手回复等字段。

## 7. 为何选择vLLM部署服务？KV-cache如何帮助推理加速？你自己做过哪些优化？
1. 基于vLLM的推理服务： 选择vLLM作为推理服务，因为它在处理长序列时具有较高的效率和吞吐量。
2. PagedAttention: KV-cache优化： 利用vLLM的KV-cache机制，缓存已计算的键值对，避免重复计算，显著提高推理速度。
- 分页缓存： 将缓存分为多个页面，每个页面包含一定数量的键值对。当模型需要计算新的键值对时，先检查缓存中是否已存在，若存在则直接使用，避免重复计算。
- 动态更新： 模型在运行过程中，会动态更新缓存中的键值对。当缓存满时，会根据最近最少使用（LRU）策略替换旧的键值对。
3. 批量处理： 对输入进行批量处理，减少模型调用次数，提高效率。
4. 异步处理： 采用异步模型，充分利用服务器资源，处理多个请求。

**自己可以尝试的优化**
- 模型并行： 利用多GPU并行计算，提高模型推理速度。
- 缓存优化： 对常用的输入进行缓存，避免重复计算。
- 批量处理： 对输入进行批量处理，减少模型调用次数，提高效率。
- 显存碎片整理： 及时释放不再使用的显存，避免内存碎片问题。
- KV量化： 对模型的键值对进行量化，减少显存占用，提高推理速度。


## 8. RAG的chunk划分策略是什么？
1. 基于内容的划分： 按段落、句子或固定长度等方式划分文档内容，每个部分作为一个chunk。
2. 基于语义的划分： 利用语义分析技术，将文档内容根据语义关系进行划分，确保每个chunk的语义连贯。
3. 混合划分： 结合基于内容和基于语义的划分方法，根据具体场景选择合适的划分策略。


## 9. 在构造偏好数据时，用同一个prompt采样多个response，怎么保证多样性？
1. 采样参数： 调整温度（temperature）和Top-p（nucleus sampling）参数，控制模型生成的随机性。较高的温度会增加随机性，而较低的Top-p会限制生成的 tokens 范围，从而增加多样性。
2. 不同的种子（seed）： 为每个采样设置不同的随机种子，确保每次生成的结果不同。
3. 人工筛选： 对生成的 response 进行人工筛选，保留符合预期的样本，过滤掉不相关或重复的样本。


## 10. 在构建偏好数据时用了聚类方法筛选高质量样本，为什么没选KMeans?
1. KMeans不符合偏好样本空间
- 簇形状假设过强： KMeans假设样本分布在欧氏空间中，而偏好样本空间可能是一个高维空间，簇形状假设过强，导致聚类结果不符合预期。
- 对异常值敏感： KMeans对异常值敏感，可能会将异常值分配到错误的簇中。
- 需要预设K： K值很难确定
2. 聚类方法
- 基于距离的聚类： 如KMeans、DBSCAN等，根据样本之间的距离进行聚类。
- 基于密度的聚类： 如DBSCAN、OPTICS等，根据样本的密度分布进行聚类。
- 基于层次的聚类： 如Agglomerative Clustering、Divisive Clustering等，通过合并或分裂样本进行聚类。


## 11. 自主构建的评估体系里如何分离知识幻觉与推理幻觉？
1. 知识幻觉： 模型生成的回复中包含了与训练数据中不存在的知识或信息。源于模型内部存储的事实性知识错误或缺失。
2. 推理幻觉： 模型在生成回复时，基于训练数据中的模式进行推理，而不是基于真实的知识。源于模型在逻辑链条上的断裂或跳跃。

方法一：基于知识图谱的反向验证
1. 构建知识图谱： 利用已有的知识库，构建一个知识图谱，将知识实体和关系表示为图结构。
2. 验证回复： 对模型生成的回复，利用知识图谱进行反向验证。检查回复中是否包含了训练数据中不存在的知识或信息。
3. 筛选样本： 根据验证结果，筛选出符合预期的样本作为训练数据。

方法二：干扰项注入与上下文遮蔽
1. 干扰项注入： 为模型输入添加随机干扰项，例如添加噪声、替换某些 tokens 等，以破坏模型对真实知识的依赖。
2. 上下文遮蔽： 对模型输入中的关键信息进行遮蔽，例如替换为占位符或特殊 tokens，以防止模型基于上下文进行推理。
3. 评估模型输出： 对注入干扰项和遮蔽上下文后的模型输出进行评估，判断是否存在知识幻觉或推理幻觉。

方法三：构建专门的“反常识/伪知识”数据集
1. 收集数据： 从已有的知识库中收集包含常识性错误或伪知识的样本。
2. 标注数据： 对收集到的样本进行人工标注，标记出其中的知识幻觉和推理幻觉。
3. 训练模型： 利用标注好的数据集，训练一个分类模型，用于判断模型生成的回复是否存在知识幻觉或推理幻觉。

## 12. 做Prompt优化时，是如何判断优化后的Prompt在Agent推理链路中性能提升的？用什么指标来衡量？
**控制变量与A/B测试**
1. 控制变量： 确保其他因素（如模型、数据集等）保持不变，仅关注Prompt优化的效果。
2. A/B测试： 对优化前和优化后的Prompt分别进行A/B测试，对比模型生成的回复质量。常用的指标包括回复质量（如BLEU、ROUGE等）、用户满意度等。

**衡量指标**
1. 结果指标
- 用户满意度： 收集用户对模型回复的满意度调查，评估用户是否对回复满意。
- 任务完成率： 统计模型在完成特定任务（如信息检索、问题回答等）的样本比例。
- 回复质量（如BLEU、ROUGE等）： 评估模型生成的回复与人工标注的参考回复之间的相似度
- 答案准确率： 统计模型在回答问题时，正确回答的样本比例。
- 鲁棒性得分： 评估模型在处理不同输入时的稳定性和可靠性，常用的指标包括模型在不同领域、不同任务上的表现。
2. 过程指标
- 工具调用准确率： 统计模型在调用外部工具（如搜索引擎、数据库等）时，成功调用的比例。
- 无效轨迹率 / 死循环率： 统计模型在生成回复时，出现无效轨迹（如重复调用相同工具、陷入循环等）的样本比例。
- 推理效率： 评估模型在处理单个请求时的响应时间，包括模型推理时间和后处理时间，平均推理步数
- 轨迹连贯性得分： 评估模型生成的回复是否连贯，是否符合逻辑链条。常用的指标包括BLEU、ROUGE等。
3. 效率与经济指标
- 推理成本： 评估模型在处理单个请求时的成本，包括模型推理成本和后处理成本。
- 系统吞吐量： 评估模型在单位时间内处理的请求数量，通常用每秒请求数（Requests per Second, RPS）表示。

## 13. Agent的设计逻辑，问创新方法的实现
1. 认知架构创新： 从线性执行到元认知循环
- 线性执行： 传统的Agent执行流程是线性的，即按照固定的顺序执行任务。
- 元认知循环： 引入元认知循环，使Agent能够在执行任务时，根据任务需求和环境变化，动态调整执行策略。
- 实现快慢双系统
    - 快思考系统： 引入快思考系统，使Agent能够在短时间内对任务进行分析和决策，提高任务完成效率。
    - 慢思考系统： 引入慢思考系统，使Agent能够在处理复杂任务时，进行深度思考和推理，解决更复杂的问题。
2. 控制流创新：从顺序流到动态图
- 顺序流： 传统的控制流是顺序执行的，即按照固定的顺序执行任务。
- 动态图： 引入动态图，使Agent能够根据任务需求和环境变化，动态调整控制流。
- 实现方式：递归规划与自我修订
    - 递归规划： 利用递归函数，根据任务需求和环境变化，动态生成控制流。
    - 自我修订： 利用Self-Reflection机制，使Agent能够在执行过程中，根据反馈和错误，自我修正控制流。
3. 记忆机制创新： 从短期缓存到结构化记忆
- 短期缓存： 传统的记忆机制是短期缓存，即只保留最近的任务信息。
- 结构化记忆： 引入结构化记忆，使Agent能够根据任务需求和环境变化，动态调整记忆内容。
- 实现方式： 利用向量数据库，将任务信息和环境信息表示为向量，存储在数据库中。在执行任务时，根据任务需求和环境变化，从数据库中检索相关信息。
4. 协作模式创新：从单打独斗到多智能体辩论
- 单打独斗： 传统的协作模式是单打独斗，即每个智能体独立执行任务。
- 多智能体辩论： 引入多智能体辩论，使Agent能够在处理复杂任务时，与其他智能体合作，解决更复杂的问题。
- 实现方式： 利用分布式系统，将多个智能体部署在不同的节点上，通过通信协议（如RPC）实现智能体之间的协作。
5. **推理范式创新： 代码即思维**
- 传统逻辑： 自然语言思考
- 创新逻辑：编程语言思考
- 实现方法： 代码链

## 14. 如果召回的答案不是想要的，该怎么处理？
1. 检索阶段的阈值与重排： 调整检索阶段的阈值，筛选出与任务相关的知识。同时，利用重排模型，对检索到的知识进行排序，提高与任务相关的知识的排名。
2. Prompt内嵌入校验逻辑： 在Prompt中嵌入校验逻辑，判断模型回复是否符合任务要求。如果不符合，触发重新召回机制。
- 重新召回： 利用RAG技术，重新召回与任务相关的知识。
3. 基于识别结果的多路径决策
- 完全无关 -> 友好拒绝与澄清
- 部分相关 -> 回答已知部分 + 指出未知
- 检索失败 -> 意图改写与二次检索
4. 引入评估-行动循环
5. 主动追问与澄清
6. 反馈机制： 引入用户反馈机制，收集用户对模型回复的满意度调查，评估用户是否对回复满意。
- 模型调整： 根据用户反馈，调整模型参数，优化模型生成的回复质量。

# 2. 技术选型与深度对比
考察点：是否具备独立技术判断力，能清晰阐述不同方案优劣及特定场景下的选型依据。

## 1. 对比Qwen3与DeepSeek-R1的模型架构，他们在设计上有哪些关键的技术路径差异？
1. Qwen3 通过动态稀疏激活和混合专家架构在保持性能的同时大幅度降低推理成本
- **核心架构**： 动态稀疏混合专家架构 Dynamic Sparse MoE, 专家激活比例动态调整至50%~70%， 175B参数规模下推理FLOPS降低42%
- **注意力机制**： Transformer-XL 改进架构 + 动态位置编码 + 滑动窗口注意力，支持256K tokens超长上下文
- 推理优化：混合精度量化（FP16/INT8动态切换）+ 投机解码，推理吞吐量提升3倍，INT8模式下精度损失<1%
- 训练优化：三阶段预训练（通用基础→推理增强→长上下文）+ 四阶段后训练（Long-CoT冷启动→推理RL→思维模式融合→通用RL）
- 数据处理：36T tokens预训练语料，覆盖119种语言；通过Qwen2.5系列模型大规模合成数据，并采用双阶段过滤（查询+响应）确保质量
- 多模态能力： 原生支持文本、图像、语音、视频四模态，视觉编码器采用改进Swin Transformer V2，COCO数据集mAP达62.3
- 思维控制： “混合思维”模式：同一模型可切换即时回答与逐步推理，支持“思维预算”控制
2. DeepSeek-R1 聚焦于深度推理能力的突破，通过大规模强化学习和独特的注意力机制优化复杂任务的逻辑链条
- **核心架构**：大规模稀疏MoE架构，总参数671B，但每个Token仅激活37B参数， 256个路由专家中激活8个+共享专家
- **注意力机制**： 多头潜在注意力（MLA），显著减小KV缓存大小，但注意力算术成本较高（在4K上下文时注意力FLOP与参数FLOP相当）
- 推理优化：动态精度调整（简单任务INT8提速3倍，复杂任务FP16保精度）+ GRPO强化学习，推理速度较密集架构提升40%
- 训练优化：两阶段训练：DeepSeek-V3预训练（14.8T tokens，266万H800小时）+ R1强化学习（基于规则的GRPO算法）
- 数据处理： 预训练数据14.8T tokens，强化学习阶段基于可验证任务（代码单元测试、数学答案校验）构建奖励信号
- 多模态能力： 支持文本、图像、代码三模态联合训练，视觉问答VQA v2.0准确率92.3%，但需分模态调用API
- 思维控制： **专注深度推理**，通过RL训练出完整的思维链（CoT）能力，在数学、逻辑等任务中展现长链推理

## 2. GRPO和PPO在RLHF中的核心区别是什么？为什么选择GRPO而不是PPO？
- GRPO: Group Relative Policy Optimization 群体相对策略优化
- PPO: Proximal Policy Optimization 近端策略优化

**核心区别**： Critic （评价模型） vs. 无 Critic

**PPO** 是一种典型的Actor-Critic架构
- 工作方式：
  - Actor 策略模型： 负责生成文本
  - Critic 评价模型： 用于评估某个状态下未来的预期总收益，用于计算优势函数
- 缺点：
  - 显存开销大： 同时加载Actor和Critic两个大模型
  - 训练不稳定： Critic作为辅助模型，其本身的预测误差会引入偏差，导致训练过程不够稳定，且调参较为复杂

**GRPO** 无Critic，依靠群体采样
- 工作方式：
  - 针对当前状态（Prompt），让旧策略生成一组多个输出样本
  - 利用这些样本的得分计算出群体的均值和标准差
  - 用这个群体的统计量来对单个样本的奖励进行标准化，从而判断某个输出在群体中的相对优劣
- 优点：
  - 节省资源
  - 降低偏差：不再依赖一个可能不准确的Critic模型来计算优势，而是通过实际采样结果来进行比较，使优势估计更贴近真实情况

**选择GRPO的原因**
1. 显著降低显存占用与训练成本
2. 更稳定的训练信号：
- PPO的Critic模型使动态学习的，容易出现Reward Hacking；
- GRPO 不依赖于绝对数值，在奖励函数尺度不一的情况下更加稳健
3. 更符合语言生成的采样特性
4. 简化RLHF流程


## 3. LoRA和全参微调相比，在推荐场景下各自的优缺点是什么？
- LoRA （Low-Rank Adaption， 低秩适应）
  - 核心思想是冻结预训练的原有参数，然后在模型的某些层旁添加一个可训练的低秩矩阵，训练时只更新这个小矩阵
  - 优点：轻量级多场景部署（大模型统一底座+小插件灵活适配）， 缓解灾难性遗忘，训练更稳定，收敛更快，，推理无额外延迟
  - 缺点：低秩假设的限制，LoRA的表达能力可能不如全参微调，选择插入位置有门槛，难以改变底层特征
- Full Fine-tuning 全参微调
  - 优点： 理论上限高，无信息丢失，简单直接
  - 缺点： 灾难性遗忘，训练成本极高，存储成本巨大， 过拟合风险


## 4. DPO相比SFT，有哪些优劣？他在Agent任务上效果提升明显吗？

**DPO （Direct Preference Optimization，直接偏好优化）**

**核心逻辑的差异**
1. SFT： 模仿“正确答案”， 本质是最大似然估计，通过大量人工标注的“输入-输出”对，训练模型去模仿标准答案。
2. DPO： 区分“好与坏”， 基于偏好的优化算法，，直接利用偏好数据优化策略，给定输入，模型要更倾向与生成人类偏好的回答，同时疏远不被偏好的回答。

**DPO相比SFT的优势**
1. 更精准的对齐：
- SFT适合学习”格式“和”知识”，无法理解“为什么这个回答好”；
- DPO显示地告诉模型”好回答“和”坏回答”的区别，鼓励模型在概率空间中拉大两者的距离
2. 缓解曝光偏差 Exposure Bias
- SFT训练时总以“正确答案”作为上下文进行预测，但在推理时，模型用自己的预测作为上下文，一旦早期预测有偏差，误差会积累
- DPO的数据通常来自模型自身的采样（或旧策略的采样），模型在训练时就已经接触过自己可能犯的错误，并学习如何修正或避免，使得推理时更加鲁棒
3. 降低不良输出概率
- SFT 很难处理“拒答数据”
- DPO包含负样本，通过（Chosen，Rejected）的成对数据，模型能清晰地学会抑制不良输出的概率，实现“不做什么”的精确控制。


## 5. 为什么要在推荐系统引入RAG？知识库的数据来源和构建流程是怎样的？
主要是为了解决传统推荐模型在知识更新，数据稀疏和可解释性等方面的固有短板。通过引入外部知识库，让推荐结果不仅更精准，也更具“智能感”。


| 传统推荐的痛点	| RAG带来的核心价值 |
| --- | --- |
| 知识“截止日期”：模型的知识止于训练之时，无法感知最新的趋势变化。|	动态知识注入：通过实时检索外部知识库（如最新的新闻、商品库），让推荐内容永远保持“新鲜”。|
| 冷启动与数据稀疏：新用户因缺乏历史交互数据，难以获得精准推荐。 | 扩展知识基础：不依赖用户行为，转而利用物品的文本描述、专家评论等外部信息来理解新物品，或通过用户画像理解新用户，从而有效缓解冷启动问题。|
| 推荐“黑箱”：推荐结果缺乏解释，用户不知道为什么推荐这个，降低了信任感。	| 增强可解释性：可以检索并展示推荐的理由，例如“因为您最近关注的某篇评测提到了这款产品”，从而提升用户体验和信任度。|
| 语义理解不足：难以理解用户模糊或复杂的查询，比如“适合送爸爸的、有情怀的生日礼物”。|	深度语义匹配：将用户意图和物品信息都映射到语义空间进行匹配，能更好地理解用户的潜在需求，而不仅仅是历史行为 |

**知识库的数据来源和构建流程**
1. 数据来源：专业的垂域知识，用户生成内容（用户的评论，评分，反馈，发帖等）， 动态知识（热门榜单，社交媒体趋势），用户画像（基于用户历史行为生成的结构化或文本摘要）
2. 知识库的构建流程：
- 数据加载与解析
- 数据预处理与分块
- 向量化
- 存入向量数据库并建立索引
3. 在线检索与推荐流程
- 查询理解与向量化
- 知识检索
- 结果精排与增强： 对初步检索出的K个结果，可以使用更精细的重拍序模型，进行二次打分，选出最精准的前N个
- 生成推荐：将原始用户信息，精排后的知识块，以及一个特定的指令（如“基于一下用户信息和知识，生成5个推荐理由”）整合成一个增强提示词，输入给大语言模型。
- 返回结果：大语言模型根据提供的上下文，生成最终的自然语言推荐结果或理由，返回给用户


## 6. 为什么选用这个优化器AdamW,它比SGD好在哪里？
AdamW结合了SGD的泛化能力和Adam的快速收敛速度的优点。

**Adam的缺陷**：权重衰减与学习率绑定，权重衰减项是加在最终梯度上的，导致它与学习率耦合在一起。当学习率需要调整时，权重衰减的效果也会随之改变，导致调参困难，且往往达不到理想的泛化效果。

**AdamW的改进**：解耦权重衰减，W代表Weight Decay（权重衰减）， 将权重衰减冲损失函数的梯度计算中移出，直接在参数更新时进行。
- 实现方式：
  - 先计算自适应梯度（动量+平方梯度）
  - 更新参数时，直接减去权重衰减项
- 好处：权重衰减与学习率解耦，可以独立调节，使得模型更容易找到平坦极值点，从而获得更好的泛化能力。


以下是 AdamW 相对于传统 SGD 的主要优势，以及它在哪些场景下表现更佳。

| 对比维度 | AdamW | SGD (含Momentum) | 结论 |
| --- | --- | --- | --- |
| 收敛速度 |  极快。 \n 利用一阶动量和二阶动量，每个参数都有独立的学习率，在稀疏特征（如推荐系统中的ID特征）上效果显著。 | 较慢。\n 对所有参数使用统一的学习率，需要精细的学习率调度来加速。 | AdamW 胜。\n 特别适合处理大规模稀疏数据 |
| 超参数敏感度 | 不敏感。 \n 默认参数（lr=1e-3, weight decay=0.01）在大多数任务上表现良好。	| 非常敏感。\n 学习率、动量、初始化都需要精心调整，否则容易不收敛或陷入局部极小。	| AdamW 胜。 \n 对初学者或快速迭代项目更友好。|
| 泛化能力	| 优秀。 \n 通过解耦权重衰减，有效控制了过拟合。尤其在 Transformer 架构（如BERT、GPT）中，AdamW 是标准配置。	| 理论上更强。 \n SGD 倾向于收敛到“平坦极小值”，在测试集上的泛化能力理论上限更高。 |	各有千秋。\n 在 CV 领域，精心调参的 SGD 有时仍略胜一筹；但在 NLP 和大模型领域，AdamW 是事实标准。|
| 内存占用	| 较高。 \n 需要为每个参数维护两套状态（动量和梯度平方），内存开销约为 SGD 的 2-3 倍。	| 较低。\n 即使使用动量，也只需维护一阶动量。	| SGD 胜。\n 适合在内存受限的设备上训练 |


**选择AdamW的原因**
1. 处理稀疏特征，AdamW的自适应学习率机制可以让低频特征的参数更新步长更大，高频特征的更新步长更小，从而高效学习。
2. Embeeding层的稳定训练，AdamW的解耦权重衰减能有效防止Embedding矩阵过拟合，同时保证数值稳定。
3. 与Transformer的兼容性，能保证模型的稳定收敛

**总结**
1. AdamW 解决了两个核心问题： 快（自适应学习率，收敛快，适合处理稀疏数据）和 稳（权重衰减解耦，，提高了模型的泛化能力，在tansformer架构上表现卓越）
2. 它比SGD好在哪里？
- 易用性：对超参不敏感
- 适应性；自动为不同参数调整学习率
- 针对性；对于包含海量稀疏特征，以及依赖transformer的架构，AdamW是主流工具。


## 7. 为什么选择这个特定Qwen2.5-VL-7B-Instruct的多模态大模型，而不是其他的？
选择 Qwen2.5-VL-7B-Instruct，本质上是在做一个务实的工程决策：
- “够用”：其原生能力已覆盖推荐系统所需的视觉理解、结构化输出等核心需求。
- “好用”：7B的规模确保了线上推理的速度和低延迟，满足实时推荐的要求。
- “能用”：开源的属性和极低的调用/部署成本，让项目在预算内可行。
- “有潜力”：在特定任务上通过微调可以超越大模型，甚至在空间推理等前沿领域也展现出可挖掘的潜力


## 8. 什么场景下用SFT，什么场景下用RL？
- SFT 负责让模型学会**知识和格式**， 模仿“问题-标准答案”对，学会**遵循指令**
- RL 负责让模型**学会目标和策略**，不给标准答案，只给一个最终目标，让模型自己去试错，探索出达成目标的最佳路径

**SFT适用场景**
1. 结果确定性强
2. 有现成的优质数据
3. 主要目标是格式遵循或知识注入：不需要模型进行复杂的探索或策略权衡

**RL适用场景**
1. 目标复杂且多位
2. 难以获得完美示例
3. 需要模型进行规划与探索
4. 优化人类偏好/多轮对话策略优化

## 9. 项目中用了LangGraph实现多工具调用链路，相比纯Prompt有什么优势？

**LangGraph的核心思想**是将智能体的行为建模成一个循环计算图。
1. 节点 Nodes：每一个节点可以是一个LLM调用，一个工具函数，或者一个固定的数据处理步骤。
2. 边 Edges：定义了流转逻辑。比如从A到B，或者从A到C的逻辑
3. 状态 State：一个在图中流转的、随时被更新的数据结构，记录了任务进行到哪一步，已经收集到了什么信息。

**优势**： 把“智能”和“流程”解耦。
- LLM只负责在关键的节点做决策， 而流程的控制则有确定的代码逻辑来保证。

1. **核心对比**：纯 Prompt 与 LangGraph

| 对比维度	| 纯 Prompt 方式	| LangGraph 方式	| 在推荐系统场景下的意义 |
| --- | --- | --- | --- |
| 控制流	| 隐式 & 自由。全靠模型自身的 Function Calling 能力决定“先调哪个工具，再调哪个工具”。	| 显式 & 确定。开发者用代码定义了一个状态机（图），规定了节点（工具调用/LLM交互）和边（流转条件）。	| 可靠性：推荐流程（如“先检索用户画像 -> 再召回商品 -> 最后生成理由”）是确定的，不会因为模型“抽风”而跳过关键步骤。 |
| 可复现性	| 低。同样的输入，模型可能今天先调A工具，明天先调B工具，导致结果不稳定。|	高。只要图结构不变，执行逻辑就是固定的，便于调试和A/B测试。	| 稳定性：对于线上服务，可复现性是保证用户体验一致性的基础。 |
| 复杂逻辑处理	| 弱。对于循环、分支、并行等复杂逻辑，Prompt 很难精确描述（例如：“如果第一个工具返回空，就重试另一个工具，最多3次”）。| 强。原生支持循环、条件分支、并行执行。可以轻松实现“重试”、“回退”、“并行检索”等高级模式。|	鲁棒性：在RAG中，如果向量数据库检索失败，可以自动切换到关键词搜索（BM25），这种 fallback 逻辑用 LangGraph 实现天衣无缝。|
| 状态管理	| 无状态。每次调用都是独立的，多轮交互的状态（如已调用的工具、已收集的信息）全靠拼接历史记录，容易超出上下文窗口。	| 有状态。Graph 维护一个全局的 State 对象，所有节点都可以读写。状态清晰，不易丢失。	| 多轮对话推荐：在连续对话中，能记住“用户刚才否定了某类商品”，并在后续工具调用中排除这类商品。|
| 可观测性	| 黑箱。很难知道模型“正在想什么”，为什么调这个工具。调试困难。	| 白箱。每一步执行到哪个节点，State 发生了什么变化，都可以被记录和追踪。	| 可解释性与调试：可以精确追踪推荐理由的生成过程：“哦，是因为调用了‘用户历史兴趣’工具，找到了他最近看的商品。” |
| 性能与成本	| 不可控。模型可能产生无效的“思考”或反复调用，浪费 tokens 和时间。	| 可控。开发者可以设定超时、最大循环次数，避免模型陷入死循环或过度消耗。| 成本优化：避免模型在无效的 tool call 上浪费 tokens，降低推理成本。|


## 10. vLLM的核心优势是什么？它是如何通过PagedAttention提升显存利用率的？

| 核心优势	| 关键技术与描述 |
| --- | --- |
| 极致吞吐量	| 通过 PagedAttention、连续批处理 和 CUDA Graph 等优化，吞吐量可达 HuggingFace 的 24倍，TGI 的 3.5倍。|
| 高效显存管理 | PagedAttention 将 KV Cache 分块存储，按需动态分配，几乎消除内存碎片，支持更长的上下文和更大的批次。|
| 灵活易用	| 与 HuggingFace 模型无缝集成，提供兼容 OpenAI 的 API 服务，开箱即支持多种解码算法和量化技术（GPTQ、AWQ、FP8等）。|
| 大规模部署	| 原生支持张量并行、流水线并行等多 GPU/多节点分布式推理，并提供基于 Docker 的便捷部署方案 |

**PagedAttention是如何提升显存利用率的？**
1. 传统方案的缺陷：预分配与内存碎片
2. PagedAttention的核心创新：将操作系统中的虚拟内存分页思想引入到KV cache管理中
- 分块 Block：不再预留一大块连续空间，而是将KV cache划分成许多固定大小的块 block，每个块可以存储固定数量token的KV向量
- 非连续存储：一个请求的逻辑连续KV cache可以被存储在物理上非连续的多个块中
- 块表 Block Table 映射：系统维护一个块表，记录每个逻辑块对应的物理块地址
3. PagedAttention带来的提升：
- 按需分配，消除碎片
- 高效的内存共享
- 更大的批处理大小


## 11. 了解哪些agent开发框架？例如 langchain、LlamaIndex、LangGraph、Qwen-Agent，他们的核心应用场景有何不同？

| 框架	| 核心定位 |	一句话概括 |
| --- | --- | --- |
| LangChain	| LLM应用开发工具箱 & 生态底座	| 什么都能接，什么都能连，快速起步的首选。|
| LangGraph	| 复杂Agent流程的控制中枢	| 把业务流程画成一张图，让机器按图索骥，适合需要循环、分支的复杂任务。|
| LlamaIndex | 知识检索的专家 (RAG) | 专治各种“数据看不懂、记不住、查不准”的毛病。|
| Qwen-Agent | 阿里生态下的工具执行与调度 | 在通义千问模型体系内，把工具调用这件事做到极致、做到最稳 |


## 12. 如何做微调的？直接用PEFT库，还是用LlamaFactory做的？和ms-Swift的区别？
1. PEFT ： Hugging Face官方出品的参数高效微调库，实现LoRA，Prefix Tuning， IA3等主流方法
- **核心特点**：
  - 提供标准的Python API，需要写代码调用
  - 与Transformers， Accelerate深度集成
  - 极致轻量，只关注PEFT方法本身

2. LlaMaFactory ： 开源大模型微调平台，主打模块化与开箱即用，提供webui界面
- **核心特点**：
  - 提供可视化Web界面，无需写代码即可微调
  - 内置上百种预训练模型支持
  - 支持全量微调、LoRA、QLoRA等多种方法
  - 中文社区活跃

3. MS-Swift： 全链路工具链， 阿里达摩院出品的多模态大模型全栈工具链，覆盖微调、推理、部署全流程
- **核心特点**：
  - 支持500+模型
  - 全流程覆盖：数据预处理，训练调度，模型压缩，服务部署
  - **多模态支持**：集成视觉编码器支持图文联合推理
  - 推理吞吐量提升20倍
  - 跨平台兼容：适配NVIDIA、昇腾、寒武纪等多硬件

**核心对比**

| 对比维度	| PEFT	| LLaMA Factory	| MS-Swift |
| --- | --- | --- | --- |
| 定位层级 | 底层核心库 | 上层集成平台 | 全链路工具链 |
| 上手难度 | 需要写代码 | 零代码（WebUI） |	中等，API简洁但功能丰富 |
| 功能范围 | 仅PEFT方法 | 微调+评估+推理+导出 | 数据预处理+训练+压缩+部署+监控 |
| 模型支持 | 所有HF模型 | 上百种内置 | 500+，特别优化中文模型 |
| 多模态能力 |	无 |	支持文本+图像/音频（通过Adapter）| 强，原生支持多模态 |
| 可视化 |	无 |	✅ WebUI界面 |	部分（训练监控）|
| 部署支持 | 无 | 有模型导出功能 | 完善，API服务、容器化、监控指标 |
| 社区生态 | HF官方，全球最广 | 中文社区活跃，53K Star | 阿里ModelScope生态，国内资源丰富 |

**MS-Swfit VS. LlamaFactory**

1. 多模态支持能力
- MS-Swift：原生支持多模态大模型微调，集成视觉编码器，支持图文联合推理
- LLaMA Factory：通过轻量化Adapter支持多模态，但非原生

2. 部署链路完整度
- MS-Swift：从数据预处理→训练→量化→服务化部署→监控，全链路覆盖
- LLaMA Factory：聚焦训练阶段，导出模型后需自行处理部署

3. 硬件适配
- MS-Swift：跨平台兼容NVIDIA、AMD、昇腾、寒武纪
- LLaMA Factory：主要支持NVIDIA GPU

4. 推理优化
- MS-Swift：推理吞吐量提升20倍，有动态批处理、算子融合等深度优化
- LLaMA Factory：无专门推理优化

## 13. Agent的工具tool的设计，是否是workflow形式？

| 维度 | 传统的工具设计视角 | 工作流视角下的工具设计 |
| --- | --- | --- |
| 核心定位 | 独立的API函数，由LLM自主决定是否调用及调用顺序。 | 工作流中的节点（Node），是整个流程图的一个环节。|
| 控制方式 | 隐式控制：模型通过ReAct等推理框架自主决定调用哪个工具，行为难以预测。 | 显式控制：开发者用有向图定义流程，LLM只在特定节点决策，整体行为可控。|
| 状态管理 | 无状态：每次调用独立，多轮工具交互的状态需手动拼接历史。 | 有状态：共享全局的状态（State）对象，所有工具节点都可读写，信息自然流转。|
| 复杂逻辑处理 | 困难。难以实现条件分支、循环、并行、重试等复杂逻辑。 | 原生支持。通过条件边（Conditional Edge）、循环、并行等机制，优雅实现复杂逻辑。|
| 可观测性 | 黑箱。很难追踪模型为何调用某个工具，调试困难。 | 白箱。每一步执行都可被记录和追踪，能精确看到工具调用链和状态变化 |

**Workflow vs. Agentic Workflow**

| - | 工作流 (Workflow)	| 智能体工作流 (Agentic Workflow) |
| --- | --- | --- |
| 思维模型 | 流程图 | 有状态循环图 + 自主决策节点 |
| 控制方式 | 代码控制 | 边界定义 + 智能体决策 |
| LLM角色 | 被调用的函数 | 决策者和行动者 |
| 可靠性 | 高，可预测 | 中，需精心设计边界 |
| 灵活性 | 低 | 高 |
| 适用场景 | 流程确定、需稳定执行的场景 | 复杂开放、需动态适应的场景 |


## 14. 多工具调用中如何用DAG（有向无环图）实现并行调度优化？
**DAG有向无环图**的核心思路是：将复杂的多工具调用任务建模成一张图，通过分析工具之间的数据依赖关系，自动识别出哪些工具可以同时运行，从而最大化并行度、最小化总执行时间。

**DAG** 有节点（工具调用）和边（依赖关系）组成。并行调度的核心算法是拓扑排序：将图按依赖关系分层，同一层内的节点相互独立，可以并行执行。

## 15. DeepSeek-R1的MLA是如何节约KV cache的？

**低秩联合压缩**

1. 压缩Down-projection： 对输入的每个token，生成一个很小的潜在向量 C_kV，它包含了Key和Value的核心信息；
2. 缓存 KV Cache：只缓存这个小的潜在向量C_KV，而不是完整的key 和 value，从而大幅度减少显存占用；
3. 解压缩 Up-porjection：在推理的生成阶段，从缓存中读取C_KV，通过一个上投影矩阵，将其恢复成完整的Key和Value，再进行注意力计算

| 方面 | 传统MHA (如Llama架构) | DeepSeek MLA |
| --- | --- | --- |
|存储内容 | 直接缓存每个注意力头的完整Key矩阵和Value矩阵。 | 通过一个“压缩矩阵”W_DKV，将Key和Value合并压缩成一个小的潜在向量 C_KV 进行缓存。|
| 存储大小 | 大。KV缓存大小与隐藏层维度 embed_dim 成正比：2 × embed_dim × layers × seq_len。 | 小。KV缓存大小与一个更小的潜在维度 latent_dim 成正比：latent_dim × layers × seq_len。 |
| 如何使用 | 直接使用缓存的Key和Value计算注意力。	| 在计算注意力之前，通过一个“解压缩矩阵”W_UK 和 W_UV，将潜在向量 C_KV 恢复（上投影）回完整的Key和Value矩阵，然后参与计算 |

**解耦RoPE**：旋转位置编码RoPE对位置信息敏感，直接压缩它会干扰模型对词序的理解。 MLA的解决方案是为位置信息单独创建了一小段独立的Query和Key向量，对它们施加RoPE，然后再与主向量拼接。解决了低秩压缩与RoPE不兼容的问题

**算子融合**：MLA将“解压缩Key的矩阵”与”Query矩阵“预先相乘，形成一个融合后的新Query矩阵，，在计算注意力分数时，Query可以直接和这个融合后的矩阵运算，仿佛Key从未被压缩过一样，完全消除了因压缩而增加的额外计算步骤，保持了推理的高效性。


## 16. 长上下文处理：超长上下文是怎么实现的？如KIMI的上下文窗口是128K，你是如何处理的？

| 对比维度 | Kimi的技术路线	| DeepSeek的技术路线 |
| --- | --- | --- |
| 核心架构 | 混合线性注意力（Kimi Linear + KDA）	| 改进型Transformer（MLA + 稀疏注意力）|
| 计算复杂度	| O(n) 线性复杂度，理论上是Transformer的突破	| 近似O(n log n)，通过稀疏化降低计算量 |
| KV缓存优化 | 减少75% 的KV缓存需求 | MLA机制将KV压缩到低维潜在空间，减少75%以上 |
| 代表模型 | Kimi Linear 48B-A3B，Kimi k1.5 | DeepSeek-V3.2-Exp（1M上下文），DeepSeek-R1 |
|核心优势 | 理论计算效率更高，解码速度提升6倍	| 与现有Transformer生态兼容性好，1M token实测稳定 |

1. KIMI

**KDA （Kimi Delta Attention）**
- **细粒度遗忘门控**：在每个通道维度上独立控制记忆保留，把重要信息留下，冗余信息扔掉，解决了传统线性注意力“记不住”的问题。
- **改进的Delta Rule**：在数学上保证状态更新的稳定性，即使在百万级token序列中，梯度也不会爆炸或消失。
- **Diagonal-Plus-Low-Rank（DPLR）结构**：把注意力矩阵拆成“对角块+低秩补丁”，让GPU并行计算时能一次性处理更多内容，吞吐率直接翻倍

Kimi Linear **采用3:1的混合层设计**： 每3层线性注意力KDA后加一层全注意力
- 线性层负责高效处理长序列，降低计算成本
- 全注意力层保留全局语义的建模能力，确保模型智商不下降
- 结果：在相同训练条件下，首次超越了全注意力模型的性能

Kimi团队还**砍掉了传统的RoPE位置编码**，让KDA自己通过时间衰减核函数学习序列位置信息。实验证明，没有RoPE，模型反而更稳、更泛化。同时，他们采用**分块并行计算**和**kernel fusion优化**，极大地减少了显存I/O开销

2. DeekSeek

- 将Key和Value压缩到低维潜在空间后再缓存
- KV缓存大小从 2 × embed_dim 降低到 latent_dim，减少75%以上
- 通过权重吸收（Weight Absorption） 技术消除解压缩带来的计算开销
- **DSA （DeepSeek Sparse Attention）**， 核心思想是：
  - 实现细粒度的稀疏注意力计算，每个token只与部分相关token交互
  - 在几乎不牺牲模型输出质量的前提下，显著提升长文本处理效率
  - 经多项公开评测集测试，性能与前代版本基本相当


## 17. 幻觉与幅度： 什么是大模型的幻觉？如何缓解？为什么会有幅度问题，业内有哪些解决办法？

1. **大模型的幻觉**： 指模型生成的内容与事实不符、与上下文矛盾或纯粹是无中生有，但表达方式却听起来自信满满、合情合理。

| 类型 | 定义 | 电商场景举例 |
| --- | --- | --- |
| 事实性幻觉 | 与客观事实不符 | 说"iPhone 15有5000mAh电池"（实际约3300mAh）|
| 上下文幻觉 | 与对话历史矛盾 | 刚才说"这件衣服有红色"，现在说"只有蓝色" |
| 逻辑/语义幻觉 | 逻辑不通或自相矛盾 | "这款鞋防滑，但建议在湿滑地面小心穿着" |

**产生幻觉的原因**：
- 统计学习本质：模型是"续写器"，不是"数据库"。它学习的是词与词之间的统计规律，而不是事实的真伪
- 知识截止边界：训练数据有截止时间（如我的是2025年5月），之后的新知识无法覆盖
- 压缩损失：训练过程是把海量知识"压缩"到有限参数中，必然有信息丢失
- 解码策略影响：temperature>0时引入随机性，可能在不确定区域"创造性发挥"

**解决幻觉的方案**

- 让模型学会“不知道”

| 方法	| 原理	| 效果	| 适用场景 |
| --- | --- | --- | --- |
| 强化学习+惩罚	| 对幻觉输出给予负向奖励 | ⭐⭐⭐ | 通用对话，需大量标注 |
| 对照解码 | 对比多个解码路径，选择一致性高的 | ⭐⭐⭐ | 生成任务，无需训练 |
| 受控解码 | 在解码时施加约束（如词汇表过滤）| ⭐⭐ | 结构化输出场景 |

- 在数据中注入“我不知道”样本： 教会模型在知识边界处“拒绝回答”， 而不是“编造答案”
- **用RAG做事实锚点** （目前工业界最有效的方案）
- 让模型自我反思： 自洽性检查，验证链检查
  - 自洽性检查 self-consistency
    - 多次采样生成多个答案
    - 选择出现频率最高或逻辑最一致的
    - 如果分歧大，说明模型不确定，可触发重新检索或承认无知
  - 验证链 Chain-of-Verfication
    - 先生成初步答案
    - 然后生成验证问题（"如何确认这个信息的准确性？"）
    - 执行验证（如检索、计算）
    - 根据验证结果修正答案

2. **幅度问题**： 也称"置信度失控"或"校准失效"，是指模型对输出的自信程度与真实正确性不匹配。

| 类型 | 表现 | 危害 |
| 过度自信 | 错的答案给出100%确定语气 | 误导用户决策 |
| 信心不足 | 对的答案也"可能、也许、大概" | 降低用户体验和信任 |

**幅度问题的原因**：
- Softmax的先天缺陷：传统Softmax输出的"概率"是相对概率，不是绝对置信度。即使模型完全不知道答案，它也会强行归一化出一个分布
- 训练目标的错位：模型训练时优化的是"下一个词预测准确率"，而不是"知道什么不知道"
- 数据分布偏差：预训练数据中很少出现"我不确定"的表达，模型没学会承认无知

**缓解幅度的解决方案**
- 模型层：置信度校准， 比如训练后调整Softmax温度参数，引入参数不确定性
- 解码层：带拒识的生成，设定一个置信度阈值，当模型对下一个词的预测概率低于阈值时，触发"拒识"流程
- 系统层：多模型投票
- 交互层：让用户参与判断

## 18. 你在大模型训练中遇到的困难，如何解决？
1. 数据问题
2. 训练不稳定： 奖励函数无效果
3. 上下文建模
4. 算力利用率
5. 评估指标

| 层面 | 核心挑战	| 关键解法 |
| --- | --- | --- |
| 数据	| 质量、配比、时效 | 清洗50%时间 + 分层混合 + 增量更新 |
| 算法	| 稳定性、长上下文	| 梯度裁剪 + 渐进式训练 + MLA创新 |
| 工程	| 算力利用率、成本	| 3D并行 + FP8混合精度 + 检查点压缩 |
| 评估	| 指标偏差、奖励过拟合	| 多维评测 + 红队攻击 + 奖励多样化 |


## 19. 你在项目中使用过DPO吗？和PPO相比，他有什么优缺点？请对比两者损失函数形式，并解释GRPO在训练稳定性上的优势
1. **PPO（Proximal Policy Optimization 近端策略优化）**
- Actor 模型：当前要训练的策略（生成回答）
- Reference 模型：冻结的初始策略（防止偏离太远）
- Reward 模型：学习人类偏好的打分器
- Critic 模型：估计状态价值，辅助 Actor 更新

2. **DPO（Direct Preference Optimization，直接偏好优化）** ： DPO 在最大化胜者回答的相对概率（相对于 reference），同时最小化败者回答的相对概率。
- Policy 模型（当前策略）
- Reference 模型（冻结的初始策略）
- 偏好数据（胜者/败者对）

3. **GRPO（Group Relative Policy Optimization，群组相对策略优化）**
- 对同一个 prompt，从旧策略采样 G 个输出（如 G=8）
- 计算这组输出的平均奖励作为基线
- 每个输出用"自己奖励 - 组平均奖励"作为优势函数
- 用这个优势更新策略


## 20. 什么场景需要GraphRAG，他的好处是什么？

| 问题类型	| 传统RAG表现	| GraphRAG表现	| 典型例子 |
| --- | --- | --- | --- |
| 单点事实查询	| ✅ 优秀	| ✅ 优秀	| "iPhone 15的电池容量是多少？" |
| 多文档综合	| 中等	| ✅ 优秀	| "对比各家券商2025年年报中关于AI战略的表述" |
| 多跳推理	| ❌ 困难	| ✅ 卓越	| "谁参与过NASA的自主空间机器人项目，现在还在做类似研究？" |
| 关系挖掘	| ❌ 困难	| ✅ 卓越	| "哪些药物靶点与阿尔茨海默症相关，且已有临床试验？" |
| 全局主题归纳	| ⚠️ 中等| 	✅ 优秀	| "2025年战略规划中覆盖了哪些核心主题？" |
| 溯源与合规	| ⚠️ 一般	| ✅ 可追溯	 | "请提供这个结论的证据链和原始出处 |

| 维度	| 传统RAG	| GraphRAG	| 优势说明 |
| --- | --- | --- | --- |
| 关系理解	| 文本块独立，丢失连接	| 显式建模实体关系	| 多跳推理：能回答"A与C通过B如何关联"类问题 |
| 可解释性	| 黑箱，难以追溯	| 提供图路径作为证据	| 可审计：可展示结论依赖的完整事实链 |
| 准确性	| 70-80%基准	| 最高提升20-25%	| 高精度：特别适合严肃场景 |
| 全局视角	| 局部相似性	| 全局结构理解	| 主题归纳：能总结分散文档的共同主题 |
| 知识更新	| 需重新索引	| 图结构可增量更新	| 动态性：新增实体/关系可局部更新 |


# 3. 工程化与系统设计
考察点： 解决实际工程难题的能力，关注性能，资源利用率，稳定性及长上下文处理等硬指标

1. 当输入超过模型上下文长度时，有哪些主流解决方案？

2. 在实际本地部署vLLM服务时，如何权衡上下文长度与显存占用的关系？是否应用过量化或动态批处理等技术？

3. KV Cache在长上下文推理中可能存在污染问题。你们是否设计了相应的缓存隔离或清理机制？

4. 本地部署时，如何平衡vLLM的上下文长度和显存占用？

5. 请手推一下MSE的梯度回传过程

6. Multi-Head Attention为什么要切分成多头？他能提高计算效率吗？请推导一下公式。

7. LoRA的原理是什么？ A、B矩阵是如何初始化的？

8. 项目里的Modular Agent，你能讲讲它是如何实现多步规划的吗？项目里提到了多个工具调用链路，调度策略是如何设计的？是否有异常fallback策略？

9. Agent评估体系包括哪些维度？如何衡量planning能力 vs hallucination rate？

10. 场景题：加入一个Agent推理链路包含3个工具+高频请求，系统整体延迟较高，你会如何优化？

11. GraphRAG的难点是什么？如何应对增量场景？

12. 客户输入一个软件或网页界面截图，如何通过RAG的方式帮助用户了解界面的每一个组件作用，相似的组件如图片框和视频框如何区分？

13. 如何搭建一个RAG链路来处理专业领域知识？ 如医疗、法律？

14. 大规模Agent系统在多线程、多进程场景下的资源调度策略如何设计？

15. 在高并发查询Agent系统中，你会如何优化召回和生成阶段的延迟？

16. 如果要在GPU资源有限的条件下同事提供推理和微调服务，如何做资源分配和任务调度以保证时延和吞吐？

17. LangChain的memory默认机制在多用户并发中怎么做隔离？你是如何保证线程安全的？

18. 训练LoRA模型时，你是如何选择冻结层的？依据是什么？

19. 微调Llama2你是怎么选择训练样本的？清洗的逻辑是什么？你有没有观察到哪些训练样本质量问题对模型行为有很大的影响？

20. kv cache是什么？为什么能极大地提升推理速度？

21. PPO的原理？从维护的四个model讲，再详细讲一下训练流程和损失函数各个参数含义

22. DPO和PPO的区别？

23. SFT冷启动时数据集构造需要注意哪些因素？为什么要做数据清洗与均衡采样？

24. 怎么构建SFT数据集，数据量是多少，微调方式是什么？

25. SFT数据问题不够多样化怎么办？

26. 微调时的训练数据是怎么构建的？如何保证样本多样性和质量？

# 4. 项目难点与故障排查
考察点：遇到困难时的分析和解决能力，是否独立找到根因并提出解决方案

1. 微调Qwen时发现验证集loss震荡的可能原因

2. 如果在训练DPO的过程中，正例和负例的loss都在下降，该如何解决？

3. 遇到过灾难性遗忘吗？怎么缓解的？

4. 模型量化时遇到激活值异常溢出如何调试？

5. 如何召回的答案不是想要的，该怎么处理？

6. 训练SFT模型时loss出现距离震荡，你是如何诊断并解决的？

7. LLM重复生成内容的问题如何缓解？

8. 如果子agent回复不对怎么办？反思？跳不出去怎么办？限制次数

9. Agent怎么评估效果

10. 你在大模型训练中遇到过的困难，如何解决？

11. 如果让你自己设计一个Agent， 你会怎么做？为什么？

# 5. 项目迭代与业务思考
考察点：是否具备宏观思考能力，能否将技术方案与业务价值结合，关注项目后续迭代方向

1. LLM4Rec对比传统搜推带来哪些收益？

2. 对llm4rec的后续迭代方向有什么见解？

3. 如果资源足够，有没有考虑过直接端到端训练

4. 你觉得目前大模型的上限在哪里？

5. 请介绍一下你的项目：目标是什么？用了什么基座模型？数据从哪来的？

6. RAG如果有噪声怎么办？

7. 如果Agent推理API需要低延迟响应，你会从哪些方面做系统级优化？

8. 后续迭代：对llm4rec的后续迭代方向有什么见解？

9. 业务结合： 你觉得在广告领域用大模型和传统的广告算法相比优缺点是什么？

10. 场景应用：介绍检索做的优化，具体追问子问题分解怎么做，有没有做意图识别？


# 6. 具体技术细节与参数调优
考察点：是否亲手跑过项目，具备扎实的调参经验和工程实现能力，理解每个参数对模型效果的具体影响

1. 请说明你设置的rank、alpha值，并分析他们对性能和手链速度的影响

2. LoRA微调秩设置的是多少

3. SFT的调参经验？说说你的经验

4. SFT使用的数据集，使用了多少张卡？SFT训练了多久？

5. SFT阶段如何避免对padding token计算loss？ 具体在代码中如何实现mask？

6. 你在大模型训练中遇到过的困难，如何解决？
